2025-02-16 15:17:54.228 [Test worker]  INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [kr.hhplus.be.server.integration.KafkaProducerIntegrationTest]: KafkaProducerIntegrationTest does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-02-16 15:17:54.370 [Test worker]  INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration kr.hhplus.be.server.ServerApplication for test class kr.hhplus.be.server.integration.KafkaProducerIntegrationTest
2025-02-16 15:17:54.798 [Test worker]  INFO kafka.utils.Log4jControllerRegistration$ - Registered kafka:type=kafka.Log4jController MBean
2025-02-16 15:17:54.818 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - 
2025-02-16 15:17:54.818 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -   ______                  _                                          
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -  |___  /                 | |                                         
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -                                               | |                     
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer -                                               |_|                     
2025-02-16 15:17:54.819 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - 
2025-02-16 15:17:54.821 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC
2025-02-16 15:17:54.821 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:host.name=192.168.0.23
2025-02-16 15:17:54.821 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.version=17.0.14
2025-02-16 15:17:54.821 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.vendor=Homebrew
2025-02-16 15:17:54.821 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.home=/opt/homebrew/Cellar/openjdk@17/17.0.14/libexec/openjdk.jdk/Contents/Home
2025-02-16 15:17:54.822 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.class.path=/Users/seongdo/.gradle/caches/8.11.1/workerMain/gradle-worker.jar:/Users/seongdo/Desktop/hhplus_week3/server-java/build/classes/java/test:/Users/seongdo/Desktop/hhplus_week3/server-java/build/resources/test:/Users/seongdo/Desktop/hhplus_week3/server-java/build/classes/java/main:/Users/seongdo/Desktop/hhplus_week3/server-java/build/resources/main:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson-spring-boot-starter/3.17.6/8794851785bd91370c92e1fa5880d39e0871d81/redisson-spring-boot-starter-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-actuator/3.4.1/bd347cc46f34c8b9a6f33b4d103bcfb3b11fc6f8/spring-boot-starter-actuator-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-jpa/3.4.1/f06be4354c339f3f880a5c66a6913cd2366eb225/spring-boot-starter-data-jpa-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/3.4.1/ff7227fc62338e0f6eba3f9f94c12eb952d4da95/spring-boot-starter-web-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-webmvc-ui/2.2.0/178d8ed6714d78b8b475c45bc60642a9232fcb70/springdoc-openapi-starter-webmvc-ui-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-webflux/3.4.1/9d5a081abed454d604c2bbf2ba603f74882e1443/spring-boot-starter-webflux-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-redis/3.4.1/d9a75f927b827a881d2f60df443d9741ef23fcda/spring-boot-starter-data-redis-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/3.3.1/e389d3f28fa4f08b2ac714dfeaa91724fa4d7c9f/spring-kafka-3.3.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.mysql/mysql-connector-j/9.1.0/5fb1d513278e1a9767dfa80ea9d8d7ee909f1a/mysql-connector-j-9.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/3.3.1/21754cd9387bee9c818cc78cac41c0c659326394/spring-kafka-test-3.3.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/3.4.1/ac1caa2ab4c8eaedd82abab3ed8d27a1b4ee2da8/spring-boot-starter-test-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-testcontainers/3.4.1/81996aed68fc6e87929b735ca117569fb09fec23/spring-boot-testcontainers-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/junit-jupiter/1.20.4/977e62d63e294828bfed88d1e99c2220ece8498a/junit-jupiter-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/mysql/1.20.4/dceb05d856af048ad003a2ce564a365dfa796178/mysql-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.11.4/a699f024a4a4706b36bddbeb42d499aff9e09379/junit-jupiter-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/5.14.2/3cfc377d4bb9fe729f3dd9098d9a9b27da58324a/mockito-junit-jupiter-5.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.11.4/e4c86fbe2a39c60c6b87260ef7f7e7c1a1906481/junit-jupiter-params-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.11.4/dc10ec209623986a68ea07f67cdc7d2a65a60355/junit-jupiter-engine-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.11.4/308315b28e667db4091b2ba1f7aa220d1ddadb97/junit-jupiter-api-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.11.4/21f61b123ad6ac8f7e73971bff3a096c8d8e1cd0/junit-platform-engine-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.11.4/8898eea3ed0da2641548d602c3e308804f166303/junit-platform-commons-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-launcher/1.11.4/3d83c201899d8c5e74e1a5d628eab900342a0e48/junit-platform-launcher-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-jdbc/3.4.1/307db83ee5f33fe810565cf980f73747b8f8f43b/spring-boot-starter-jdbc-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/3.4.1/c1d084f65d8d9f2de9daccab47c4f452fb0464de/spring-boot-starter-json-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/3.4.1/2c97b6fdc451ea69cd04dcfa54980439b7c7cb34/spring-boot-starter-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator-autoconfigure/3.4.1/75ab4d3c257fc5b00fbfa8099ec35b6c9702b629/spring-boot-actuator-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-jakarta9/1.14.2/50950404a99cde864c0e4ff3b1647e5be75d7570/micrometer-jakarta9-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-webmvc-api/2.2.0/3a7a3a7ecd2537203961d83cabc6d642f294ddb/springdoc-openapi-starter-webmvc-api-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/6.2.1/44bdf7e5641d44044ac52d7bb5c1fc46004e7754/spring-webmvc-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webflux/6.2.1/f8563a18b02c0aa1f7cd1ee298e76abd3514822b/spring-webflux-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/6.2.1/877acb94c5b3a0c92e652b6bebdfdc7c60922ac8/spring-web-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-jpa/3.4.1/1c704fa9169ea3745775568e733fddd0132070b2/spring-data-jpa-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/3.4.1/87ac4bbb73af12298ddf1cd6830121527e6de7c7/spring-boot-test-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/3.4.1/56fb2970279daa00359e37fabdc3bf46a1ab1a8b/spring-boot-test-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-common/2.2.0/352343daae911b5d95c718c4a3c461cf94b4707b/springdoc-openapi-starter-common-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/3.4.1/f17b54cc5816ec8f06d0aca9df11c330ead97f2a/spring-boot-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/3.4.1/77873fd387c46b7bb350cc6127a3b0162c41f9bf/spring-boot-actuator-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/3.4.1/5fb9890a5eb7c4e86c8f5c0f6960b79240daf3d5/spring-boot-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson-spring-data-27/3.17.6/5f9cce45163654e11d4f54d1699f7495d25f96c6/redisson-spring-data-27-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-redis/3.4.1/43500de2974e20b1932255a9c2ab27172a579fe6/spring-data-redis-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-keyvalue/3.4.1/7c9c27baa8b6718f3e52b7a98dbc94334c1c1d1d/spring-data-keyvalue-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context-support/6.2.1/455eb5dc0c675dc49044c1518763f4c0323a5860/spring-context-support-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/6.2.1/f56c7431b03860bfdb016e68f484c5c35531ef2e/spring-context-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-core/1.14.2/7ec567b052bc560ba76a95eff222fb7999b79817/micrometer-core-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-observation/1.14.2/a9cad29cc04c0f7e30e3e58b454d4cd47ccc54bd/micrometer-observation-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hibernate.orm/hibernate-core/6.6.4.Final/95c6d2d58c40dbbfbbd58084941e5cbca4ddef2f/hibernate-core-6.6.4.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aspects/6.2.1/50350218608abf215ae40ef00c87cc666737e199/spring-aspects-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/3.4.1/ac4bb51582c57cfb0d2beb102a76fe1a4d8b8b21/spring-boot-starter-tomcat-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.webjars/swagger-ui/5.2.0/c48d665a3f3a5d73afa34982953d3c31acc1d1dd/swagger-ui-5.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-reactor-netty/3.4.1/2a46cf5e11f1cb5c54d9091b8bb67b5caf0030ab/spring-boot-starter-reactor-netty-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.lettuce/lettuce-core/6.4.1.RELEASE/5e4483ac2281c76bd89754ce0192b3663b5cf68/lettuce-core-6.4.1.RELEASE.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson/3.17.6/15982032ba086435ec4d0d46b0e9a2c6e043b67f/redisson-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/6.2.1/35335abbdd4b1a781e6ad47d6aaa2c294d62809f/spring-messaging-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-orm/6.2.1/7fd75b4984d92c3c5e46ee94365cfc79d2daa821/spring-orm-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/6.2.1/def8d3d9bebafc36c19f8407645eddde32454c8a/spring-jdbc-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/6.2.1/5ffde4fee85ff021ad613b9e86a9be893fb52572/spring-tx-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/2.0.10/2990d2957ef0988dd243d06e04d357eace43a522/spring-retry-2.0.10.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/832245e638abf7ec3996b76a42944960abe39e23/kafka_2.13-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server/3.8.1/9b339e97deeecaf1e7897b4c731d77392b4745de/kafka-server-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator/3.8.1/d372d5bfee64939af353d4246cab795884cb477c/kafka-group-coordinator-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.8.1/3d6662afa30ed98985422d35485962819253040b/kafka-metadata-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.8.1/8f44b9f0da4fa1670d8506d33c06ba7662b37045/kafka-raft-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.8.1/daf35a6f6a1b8da8d2baa754856b12d5ebec7be3/kafka-storage-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.8.1/a522b224b598f30b9637ad2eb3a5db64c14db4a4/kafka-storage-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/483d5f635aa0a751946a6bd80b0b3acbdc6b7169/kafka-server-common-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.8.1/edc3d933c78754476fc5564f4fc2cb3da060a25a/kafka-streams-test-utils-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.8.1/9e054a4e7b88bc80f069e22045826013b7b697ec/kafka-streams-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator-api/3.8.1/9f99f11956404ae64a2b210adaf6d1f3850ab26f/kafka-group-coordinator-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-tools-api/3.8.1/43f69819f286340b5738e562122256b1b2470e99/kafka-tools-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/fd79e3aa252c6d818334e9c0bac8166b426e498c/kafka-clients-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/6.2.1/60b7fe91a4ecb88b9ee0d6f228ad0b6a8ac78cb2/spring-test-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.8.4/6638e37b887b5a279044afbdc9928e19f678eb2e/zookeeper-3.8.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.9.0/37fe2217f577b0b68b18e62c4d17a8858ecf9b69/json-path-2.9.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-oxm/6.2.1/5adc025b3f67a1931d0fb938b4fca685178672ac/spring-oxm-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-runtime/4.0.5/ca84c2a7169b5293e232b9d00d1e4e36d4c3914a/jaxb-runtime-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-core/4.0.5/7b4b11ea5542eea4ad55e1080b23be436795b3/jaxb-core-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-core-jakarta/2.2.15/6c85064b85f895b7f0c0819d950995274c0931a4/swagger-core-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/4.0.2/6cd5a999b834b63238005b7144136379dc36cad2/jakarta.xml.bind-api-4.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.5.1/4c11d2808d009132dfbbf947ebf37de6bf266c8e/json-smart-2.5.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.26.3/d26263eb7524252d98e602fc6942996a3195e29/assertj-core-3.26.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.awaitility/awaitility/4.2.2/7336242073ebf83fe034e42b46a403c5501b63c9/awaitility-4.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/jdbc/1.20.4/a02c0d9c5bf18fac3a3e27f2daec1c0137b30b3d/jdbc-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/database-commons/1.20.4/75b6aae47412a70fc8fc7154144cbb57852b2665/database-commons-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/testcontainers/1.20.4/ee2fe3afc9fa6cb2e6a43233998f3633f761692f/testcontainers-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/junit/junit/4.13.2/8ac9e16d933b6fb43bc7f576336b8f4d7eb5ba12/junit-4.13.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/2.2/3f2bd07716a31c395e2837254f37f21f0f0ab24b/hamcrest-core-2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/5.14.2/f7bf936008d7664e2002c3faf0c02071c8d10e7c/mockito-core-5.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.3/aaa43e0823d2a0e106e8754d6a9c4ab24e05e9bc/jsonassert-1.5.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/3.4.1/3ae5f19bc2b1b30de85b0610ae25818c2e7c295a/spring-data-commons-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/6.2.1/a9384de38fc00751084446ba014a0c4962240244/spring-aop-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/6.2.1/ab57ec03ba6900075bf28e3cd70ccce173205b8d/spring-beans-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/6.2.1/91fcf6b9501705c31c8337e2713fe823bb512b24/spring-expression-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/6.2.1/f42e6b51d9c0c2fcf95df9e5848470d173adc9af/spring-core-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.10.0/1355088731b4ec2107ff7f319f0d7445d916bab/xmlunit-core-2.10.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/3.4.1/5cd01e208b15113c7f88b3ea40e843ea9989f38a/spring-boot-starter-logging-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/2.1.1/48b9bda22b091b1f48b13af03fe36db3be6e1ae3/jakarta.annotation-api-2.1.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.18.2/7b6ff96adf421f4c6edbd694e797dd8fe434510a/jackson-datatype-jsr310-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.18.2/9ed6d538ebcc66864e114a7040953dce6ab6ea53/jackson-datatype-jdk8-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.18.2/72960cb3277347a748911d100c3302d60e8a616a/jackson-module-parameter-names-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.18.2/84bb533a599a4eaf901aecce5e75d49490a88774/jackson-module-scala_2.13-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.18.2/3e1f715d7cbcc5b79b104115bf8d2a7b0a9b4b22/jackson-dataformat-csv-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.18.2/deef8697b92141fb6caf7aa86966cff4eec9b04f/jackson-databind-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-api/3.4.0/9ef23dcc93693f15e69b64632be096c38e31bc44/docker-java-api-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-models-jakarta/2.2.15/664bc63998b703b3e695a00f502f6091b8c7815f/swagger-models-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.18.2/985d77751ebc7fce5db115a986bc9aa82f973f4a/jackson-annotations-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.18.2/fb64ccac5c27dca8819418eb4e443a9f496d9ee7/jackson-core-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.18.2/d000e13505d1cf564371516fa3d5b8769a779dc9/jackson-dataformat-yaml-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/2.3/936b36210e27320f920536f695cf1af210c44586/snakeyaml-2.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-commons/1.14.2/69c454dbec59c7842cf59a534b7ec03618d75b91/micrometer-commons-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.zaxxer/HikariCP/5.1.0/8c96e36c14461fc436bb02b264b96ef3ca5dca8c/HikariCP-5.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.persistence/jakarta.persistence-api/3.1.0/66901fa1c373c6aff65c13791cc11da72060a8d6/jakarta.persistence-api-3.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.transaction/jakarta.transaction-api/2.0.1/51a520e3fae406abb84e2e1148e6746ce3f80a1a/jakarta.transaction-api-2.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.6.1.Final/886afbb445b4016a37c8960a7aef6ebd769ce7e5/jboss-logging-3.6.1.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hibernate.common/hibernate-commons-annotations/7.0.3.Final/e183c4be8bb41d12e9f19b374e00c34a0a85f439/hibernate-commons-annotations-7.0.3.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.smallrye/jandex/3.2.0/f17ad860f62a08487b9edabde608f8ac55c62fa7/jandex-3.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml/classmate/1.7.0/e98374da1f2143ac8e6e0a95036994bb19137a3/classmate-1.7.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.15.11/f61886478e0f9ee4c21d09574736f0ff45e0a46c/byte-buddy-1.15.11.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.inject/jakarta.inject-api/2.0.1/4c28afe1991a941d7702fe1362c365f0a8641d1e/jakarta.inject-api-2.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.antlr/antlr4-runtime/4.13.0/5a02e48521624faaf5ff4d99afc88b01686af655/antlr4-runtime-4.13.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.5.12/3790d1a62e868f7915776dfb392bd9a29ce8d954/logback-classic-1.5.12.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.9.4/7efe6ccf593e52a2b77f98de52238f15b4a67188/jose4j-0.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.4/e5838ddcf8c1c2b612a7956b9f80423a508675db/scala-logging_2.13-3.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.1.12.1/cb2f351bf4463751201f43bb99865235d5ba07ca/metrics-core-4.1.12.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-transport-zerodep/3.4.0/c4ce6d8695cfdb0027872f99cc20f8f679f8a969/docker-java-transport-zerodep-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.24.3/da1143e2a2531ee1c2d90baa98eb50a28a39d5a7/log4j-to-slf4j-2.24.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/2.0.16/6d57da3e961daac65bcca0dd3def6cd11e48a24a/jul-to-slf4j-2.0.16.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/2.0.16/172931663a09a1fa515567af5fbef00897d3c04/slf4j-api-2.0.16.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.9.22.1/bca243d0af0db4758fbae45c5f4995cb5dabb612/aspectjweaver-1.9.22.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/10.1.34/eef6d430f34b6e393b8d9e40f10db9043732b4e5/tomcat-embed-websocket-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/10.1.34/f610f84be607fbc82e393cc220f0ad45f92afc91/tomcat-embed-core-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/10.1.34/d2b2daca3bc999c62e58ae36b45ba0582530fb25/tomcat-embed-el-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor.netty/reactor-netty-http/1.2.1/5d5fb96ccd9daf98e0c0182da8724f006119cf44/reactor-netty-http-1.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor.netty/reactor-netty-core/1.2.1/9c85dceb89ee078615ff02dd1a9afedbf3c1b14d/reactor-netty-core-1.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor/reactor-core/3.7.1/3391286019effb6c179ebbdd2159130dde543c3a/reactor-core-3.7.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns-native-macos/4.1.116.Final/d2449cd6b6b1e3f2c329f685cadf66c10d75ed28/netty-resolver-dns-native-macos-4.1.116.Final-osx-x86_64.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns-classes-macos/4.1.116.Final/216d4b543fb0e046ea819b2a39a5dd794fe9776f/netty-resolver-dns-classes-macos-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns/4.1.116.Final/9f9a00e071e8e63b1aa3db14aacb93c8ecba1ac8/netty-resolver-dns-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-http2/4.1.116.Final/5aefc3cf9ca7df764d0b15b9cb9b6e706a4a55b3/netty-codec-http2-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler-proxy/4.1.116.Final/3e0c3f519305905e41b69dc73064684e31d68357/netty-handler-proxy-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-http/4.1.116.Final/75a0455171848a4188a9f77e9a1e76a036381260/netty-codec-http-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.116.Final/eaef854ef33f3fd3d0ecf927690d8112c710bc05/netty-handler-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-dns/4.1.116.Final/6fa882dde3db7aecc7b7d4882f70f17d9e33b2ad/netty-codec-dns-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-socks/4.1.116.Final/6d67f4ad92677a29e7a5aae340cb33ea80408c72/netty-codec-socks-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.116.Final/cbe928f601e51a0b5750342b6dad5d35fa12f745/netty-codec-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.116.Final/1c6021a777b96562078d38f62ab8bd7d87627d18/netty-transport-native-epoll-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.116.Final/31ae9fc682c72ec448887b00698e4c8b275ff42d/netty-transport-native-epoll-4.1.116.Final-linux-x86_64.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.116.Final/ce620832453892a92be73f672cd79a36f8c60235/netty-transport-classes-epoll-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.116.Final/1f77d6d8e582ded200e0be1462afbc7563144e12/netty-transport-native-unix-common-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.116.Final/3fa60bbc84222a691bb0686a82308b8e180812b7/netty-transport-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.116.Final/77a86414b9c1e52622e467f1ef7ccb473996742a/netty-buffer-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.116.Final/82fc6bbf59eae483712c8716b8d1f5112b1721a0/netty-resolver-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.116.Final/6871f95af2bc3a98fda34a580baf6cac8cbc2944/netty-common-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/javax.cache/cache-api/1.1.1/c56fb980eb5208bfee29a9a5b9d951aba076bd91/cache-api-1.1.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.reactivex.rxjava3/rxjava/3.1.10/d980c75b983334eeb9a7ea1a252d044f5e6cdf77/rxjava-3.1.10.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.reactivestreams/reactive-streams/1.0.4/3864a1320d97d7b045f729a326e1e077661f31b7/reactive-streams-1.0.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.marshalling/jboss-marshalling-river/2.0.11.Final/d05ee25288b94c93c10e1d8b726305898c4fd5bf/jboss-marshalling-river-2.0.11.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.marshalling/jboss-marshalling/2.0.11.Final/d21ac137e4ab8cff5c7b66445bd03e682d865136/jboss-marshalling-2.0.11.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jodd/jodd-bean/5.1.6/5cbb398dda5c36908b123e98c373d3c1ef72f717/jodd-bean-5.1.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.6-4/ba9e303e0b5e94cdd0017390d7d8c06f47fd61f7/zstd-jni-1.5.6-4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.10.5/ac605269f3598506196e469f1fb0d7ed5c55059e/snappy-java-1.1.10.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.8.4/de7b8a41bbe1ccdfc009de51fa6d160db3ca8025/zookeeper-jute-3.8.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.12.0/e0efa60318229590103e31c69ebdaae56d903644/audience-annotations-0.12.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.5.12/65b1fa25fe8d8e4bdc140e79eb67ac6741f775e2/logback-core-1.5.12.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.14.0/a4c6e1f6c196339473cd2e1b037f0eb97c62755b/commons-io-2.14.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-transaction-coordinator/3.8.1/3c2676a95a416f3d5a59a498c4553914be1d332d/kafka-transaction-coordinator-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.pcollections/pcollections/4.0.1/59f3bf5fb28c5f5386804dcf129267416b75d7c/pcollections-4.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-validator/commons-validator/1.7/76069c915de3787f3ddd8726a56f47a95bfcbb0e/commons-validator-1.7.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.10.0/2464528d14329ff2b44bceff918d6c00300adcf4/scala-collection-compat_2.13-2.10.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.2/5b54f60ccf03e41e24c466462a72475ee918c304/scala-java8-compat_2.13-1.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.14/8e275fefb2a01e178db2cdfebb2181062a790b82/scala-reflect-2.13.14.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.15/ed6f1d58968b16c5f9067d5cac032d952552de58/scala-library-2.13.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.3.0/152ea56b3a72f655d4fd677fc0ef2596c3dd5e6e/opentest4j-1.3.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.eclipse.angus/angus-activation/2.0.2/41f1e0ddd157c856926ed149ab837d110955a9fc/angus-activation-2.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/2.1.3/fa165bd70cda600368eee31555222776a46b881f/jakarta.activation-api-2.1.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.5.1/19b820261eb2e7de7d5bde11d1c06e4501dd7e5f/accessors-smart-2.5.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.11/a38b16385e867f59a641330f0362ebe742788ed8/byte-buddy-agent-1.15.11.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.3/1049c09f1de4331e8193e579448d0916d75b7631/objenesis-3.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/6.2.1/a5d662d64470aff0ae51d210147bb6ede31a8ea3/spring-jcl-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-compress/1.24.0/b4b1b5a3d9573b2970fddab236102c0a4d27d35e/commons-compress-1.24.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.rnorth.duct-tape/duct-tape/1.0.8/92edc22a9ab2f3e17c9bf700aaee377d50e8b530/duct-tape-1.0.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.2.2/7959933ebcc0f05b2eaa5af0a0c8689fa257b15c/HdrHistogram-2.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.latencyutils/LatencyUtils/2.0.3/769c0b82cb2421c8256300e907298a9410a2a3d3/LatencyUtils-2.0.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jodd/jodd-core/5.1.6/7e3904364d0891b58a958c47336d35518084ddec/jodd-core-5.1.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/7.9.2/6409b667493149191b09fe1fce94bada6096a3e9/rocksdbjni-7.9.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.ben-manes.caffeine/caffeine/3.1.8/24795585df8afaf70a2cd534786904ea5889c047/caffeine-3.1.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.9.4/d52b9abcd97f38c81342bb7e7ae1eee9b73cba51/commons-beanutils-1.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/2.1/73a8001e7a54a255eef0f03521ec1805dc738ca0/commons-digester-2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.2/8ad72fe39fa8c91eaaf12aadb21e0c3661fe26d5/commons-collections-3.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.6/aa205cf0a06dbd8e04ece91c0b37c3f5d567546a/asm-9.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jetbrains/annotations/17.0.0/8ceead41f4e71821919dbdb7a9847608f1a938cb/annotations-17.0.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-transport/3.4.0/c058705684d782effc4b2edfdef1a87544ba4af8/docker-java-transport-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/5.13.0/1200e7ebeedbe0d10062093f32925a912020e747/jna-5.13.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.24.3/b02c125db8b6d295adf72ae6e71af5d83bce2370/log4j-api-2.24.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/txw2/4.0.5/f36a4ef12120a9bb06d766d6a0e54b144fd7ed98/txw2-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.sun.istack/istack-commons-runtime/4.1.2/18ec117c85f3ba0ac65409136afa8e42bc74e739/istack-commons-runtime-4.1.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-lang3/3.17.0/b17d2136f0460dcc0d2016ceefca8723bdf4ee70/commons-lang3-3.17.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-annotations-jakarta/2.2.15/951eda18dbce0397347056aceff38d02710ec866/swagger-annotations-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.validation/jakarta.validation-api/3.0.2/92b6631659ba35ca09e44874d3eb936edfeee532/jakarta.validation-api-3.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.37.0/ba74746d38026581c12166e164bb3c15e90cc4ea/checker-qual-3.37.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.21.1/6d9b10773b5237df178a7b3c1b4208df7d0e7f94/error_prone_annotations-2.21.1.jar
2025-02-16 15:17:54.824 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.library.path=/Users/seongdo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2025-02-16 15:17:54.824 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.io.tmpdir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.compiler=<NA>
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.name=Mac OS X
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.arch=aarch64
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.version=15.3
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.name=seongdo
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.home=/Users/seongdo
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.dir=/Users/seongdo/Desktop/hhplus_week3/server-java
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.free=19MB
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.max=512MB
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.total=80MB
2025-02-16 15:17:54.825 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.enableEagerACLCheck = false
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.digest.enabled = true
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.closeSessionTxn.enabled = true
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.flushDelay = 0 ms
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.maxWriteQueuePollTime = 0 ms
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.maxBatchSize=1000
2025-02-16 15:17:54.827 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - zookeeper.intBufferStartingSizeBytes = 1024
2025-02-16 15:17:54.829 [Test worker]  INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog - zookeeper.snapshot.trust.empty : false
2025-02-16 15:17:54.846 [Test worker]  INFO org.apache.zookeeper.server.watch.WatchManagerFactory - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-02-16 15:17:54.846 [Test worker]  INFO org.apache.zookeeper.server.watch.WatchManagerFactory - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2025-02-16 15:17:54.847 [Test worker]  INFO org.apache.zookeeper.server.ZKDatabase - zookeeper.snapshotSizeFactor = 0.33
2025-02-16 15:17:54.847 [Test worker]  INFO org.apache.zookeeper.server.ZKDatabase - zookeeper.commitLogCount=500
2025-02-16 15:17:54.850 [Test worker]  INFO org.apache.zookeeper.server.BlueThrottle - Weighed connection throttling is disabled
2025-02-16 15:17:54.851 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - minSessionTimeout set to 1600 ms
2025-02-16 15:17:54.852 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - maxSessionTimeout set to 16000 ms
2025-02-16 15:17:54.852 [Test worker]  INFO org.apache.zookeeper.server.ResponseCache - getData response cache size is initialized with value 400.
2025-02-16 15:17:54.852 [Test worker]  INFO org.apache.zookeeper.server.ResponseCache - getChildren response cache size is initialized with value 400.
2025-02-16 15:17:54.853 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.slotCapacity = 60
2025-02-16 15:17:54.854 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.slotDuration = 15
2025-02-16 15:17:54.854 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.maxDepth = 6
2025-02-16 15:17:54.854 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.initialDelay = 5
2025-02-16 15:17:54.854 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.delay = 5
2025-02-16 15:17:54.854 [Test worker]  INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.enabled = false
2025-02-16 15:17:54.856 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - The max bytes for all large requests are set to 104857600
2025-02-16 15:17:54.856 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - The large request threshold is set to -1
2025-02-16 15:17:54.856 [Test worker]  INFO org.apache.zookeeper.server.AuthenticationHelper - zookeeper.enforce.auth.enabled = false
2025-02-16 15:17:54.856 [Test worker]  INFO org.apache.zookeeper.server.AuthenticationHelper - zookeeper.enforce.auth.schemes = []
2025-02-16 15:17:54.856 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Created server with tickTime 800 ms minSessionTimeout 1600 ms maxSessionTimeout 16000 ms clientPortListenBacklog -1 datadir /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/kafka-6740444850777319969/version-2 snapdir /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/kafka-14484945198614603557/version-2
2025-02-16 15:17:54.862 [Test worker]  WARN org.apache.zookeeper.server.ServerCnxnFactory - maxCnxns is not configured, using default value 0.
2025-02-16 15:17:54.862 [Test worker]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 20 worker threads, and 64 kB direct buffers.
2025-02-16 15:17:54.865 [Test worker]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - binding to port /127.0.0.1:0
2025-02-16 15:17:54.878 [Test worker]  INFO org.apache.zookeeper.server.persistence.SnapStream - zookeeper.snapshot.compression.method = CHECKED
2025-02-16 15:17:54.879 [Test worker]  INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog - Snapshotting: 0x0 to /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/kafka-14484945198614603557/version-2/snapshot.0
2025-02-16 15:17:54.881 [Test worker]  INFO org.apache.zookeeper.server.ZKDatabase - Snapshot loaded in 14 ms, highest zxid is 0x0, digest is 1371985504
2025-02-16 15:17:54.882 [Test worker]  INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog - Snapshotting: 0x0 to /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/kafka-14484945198614603557/version-2/snapshot.0
2025-02-16 15:17:54.882 [Test worker]  INFO org.apache.zookeeper.server.ZooKeeperServer - Snapshot taken in 1 ms
2025-02-16 15:17:54.892 [ProcessThread(sid:0 cport:50088):]  INFO org.apache.zookeeper.server.PrepRequestProcessor - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2025-02-16 15:17:54.893 [Test worker]  INFO org.apache.zookeeper.server.RequestThrottler - zookeeper.request_throttler.shutdownTimeout = 10000 ms
2025-02-16 15:17:55.042 [Test worker]  INFO kafka.server.KafkaConfig - KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:50088
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-02-16 15:17:55.073 [Test worker]  INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2025-02-16 15:17:55.074 [Test worker]  INFO org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig - RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-02-16 15:17:55.163 [Test worker]  INFO kafka.server.KafkaServer - starting
2025-02-16 15:17:55.163 [Test worker]  INFO kafka.server.KafkaServer - Connecting to zookeeper on 127.0.0.1:50088
2025-02-16 15:17:55.175 [Test worker]  INFO kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:50088.
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=192.168.0.23
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=17.0.14
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Homebrew
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/opt/homebrew/Cellar/openjdk@17/17.0.14/libexec/openjdk.jdk/Contents/Home
2025-02-16 15:17:55.178 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/Users/seongdo/.gradle/caches/8.11.1/workerMain/gradle-worker.jar:/Users/seongdo/Desktop/hhplus_week3/server-java/build/classes/java/test:/Users/seongdo/Desktop/hhplus_week3/server-java/build/resources/test:/Users/seongdo/Desktop/hhplus_week3/server-java/build/classes/java/main:/Users/seongdo/Desktop/hhplus_week3/server-java/build/resources/main:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson-spring-boot-starter/3.17.6/8794851785bd91370c92e1fa5880d39e0871d81/redisson-spring-boot-starter-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-actuator/3.4.1/bd347cc46f34c8b9a6f33b4d103bcfb3b11fc6f8/spring-boot-starter-actuator-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-jpa/3.4.1/f06be4354c339f3f880a5c66a6913cd2366eb225/spring-boot-starter-data-jpa-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/3.4.1/ff7227fc62338e0f6eba3f9f94c12eb952d4da95/spring-boot-starter-web-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-webmvc-ui/2.2.0/178d8ed6714d78b8b475c45bc60642a9232fcb70/springdoc-openapi-starter-webmvc-ui-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-webflux/3.4.1/9d5a081abed454d604c2bbf2ba603f74882e1443/spring-boot-starter-webflux-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-redis/3.4.1/d9a75f927b827a881d2f60df443d9741ef23fcda/spring-boot-starter-data-redis-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/3.3.1/e389d3f28fa4f08b2ac714dfeaa91724fa4d7c9f/spring-kafka-3.3.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.mysql/mysql-connector-j/9.1.0/5fb1d513278e1a9767dfa80ea9d8d7ee909f1a/mysql-connector-j-9.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/3.3.1/21754cd9387bee9c818cc78cac41c0c659326394/spring-kafka-test-3.3.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/3.4.1/ac1caa2ab4c8eaedd82abab3ed8d27a1b4ee2da8/spring-boot-starter-test-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-testcontainers/3.4.1/81996aed68fc6e87929b735ca117569fb09fec23/spring-boot-testcontainers-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/junit-jupiter/1.20.4/977e62d63e294828bfed88d1e99c2220ece8498a/junit-jupiter-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/mysql/1.20.4/dceb05d856af048ad003a2ce564a365dfa796178/mysql-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.11.4/a699f024a4a4706b36bddbeb42d499aff9e09379/junit-jupiter-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/5.14.2/3cfc377d4bb9fe729f3dd9098d9a9b27da58324a/mockito-junit-jupiter-5.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.11.4/e4c86fbe2a39c60c6b87260ef7f7e7c1a1906481/junit-jupiter-params-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.11.4/dc10ec209623986a68ea07f67cdc7d2a65a60355/junit-jupiter-engine-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.11.4/308315b28e667db4091b2ba1f7aa220d1ddadb97/junit-jupiter-api-5.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.11.4/21f61b123ad6ac8f7e73971bff3a096c8d8e1cd0/junit-platform-engine-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.11.4/8898eea3ed0da2641548d602c3e308804f166303/junit-platform-commons-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-launcher/1.11.4/3d83c201899d8c5e74e1a5d628eab900342a0e48/junit-platform-launcher-1.11.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-jdbc/3.4.1/307db83ee5f33fe810565cf980f73747b8f8f43b/spring-boot-starter-jdbc-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/3.4.1/c1d084f65d8d9f2de9daccab47c4f452fb0464de/spring-boot-starter-json-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/3.4.1/2c97b6fdc451ea69cd04dcfa54980439b7c7cb34/spring-boot-starter-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator-autoconfigure/3.4.1/75ab4d3c257fc5b00fbfa8099ec35b6c9702b629/spring-boot-actuator-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-jakarta9/1.14.2/50950404a99cde864c0e4ff3b1647e5be75d7570/micrometer-jakarta9-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-webmvc-api/2.2.0/3a7a3a7ecd2537203961d83cabc6d642f294ddb/springdoc-openapi-starter-webmvc-api-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/6.2.1/44bdf7e5641d44044ac52d7bb5c1fc46004e7754/spring-webmvc-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webflux/6.2.1/f8563a18b02c0aa1f7cd1ee298e76abd3514822b/spring-webflux-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/6.2.1/877acb94c5b3a0c92e652b6bebdfdc7c60922ac8/spring-web-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-jpa/3.4.1/1c704fa9169ea3745775568e733fddd0132070b2/spring-data-jpa-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/3.4.1/87ac4bbb73af12298ddf1cd6830121527e6de7c7/spring-boot-test-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/3.4.1/56fb2970279daa00359e37fabdc3bf46a1ab1a8b/spring-boot-test-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springdoc/springdoc-openapi-starter-common/2.2.0/352343daae911b5d95c718c4a3c461cf94b4707b/springdoc-openapi-starter-common-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/3.4.1/f17b54cc5816ec8f06d0aca9df11c330ead97f2a/spring-boot-autoconfigure-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/3.4.1/77873fd387c46b7bb350cc6127a3b0162c41f9bf/spring-boot-actuator-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/3.4.1/5fb9890a5eb7c4e86c8f5c0f6960b79240daf3d5/spring-boot-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson-spring-data-27/3.17.6/5f9cce45163654e11d4f54d1699f7495d25f96c6/redisson-spring-data-27-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-redis/3.4.1/43500de2974e20b1932255a9c2ab27172a579fe6/spring-data-redis-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-keyvalue/3.4.1/7c9c27baa8b6718f3e52b7a98dbc94334c1c1d1d/spring-data-keyvalue-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context-support/6.2.1/455eb5dc0c675dc49044c1518763f4c0323a5860/spring-context-support-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/6.2.1/f56c7431b03860bfdb016e68f484c5c35531ef2e/spring-context-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-core/1.14.2/7ec567b052bc560ba76a95eff222fb7999b79817/micrometer-core-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-observation/1.14.2/a9cad29cc04c0f7e30e3e58b454d4cd47ccc54bd/micrometer-observation-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hibernate.orm/hibernate-core/6.6.4.Final/95c6d2d58c40dbbfbbd58084941e5cbca4ddef2f/hibernate-core-6.6.4.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aspects/6.2.1/50350218608abf215ae40ef00c87cc666737e199/spring-aspects-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/3.4.1/ac4bb51582c57cfb0d2beb102a76fe1a4d8b8b21/spring-boot-starter-tomcat-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.webjars/swagger-ui/5.2.0/c48d665a3f3a5d73afa34982953d3c31acc1d1dd/swagger-ui-5.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-reactor-netty/3.4.1/2a46cf5e11f1cb5c54d9091b8bb67b5caf0030ab/spring-boot-starter-reactor-netty-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.lettuce/lettuce-core/6.4.1.RELEASE/5e4483ac2281c76bd89754ce0192b3663b5cf68/lettuce-core-6.4.1.RELEASE.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.redisson/redisson/3.17.6/15982032ba086435ec4d0d46b0e9a2c6e043b67f/redisson-3.17.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/6.2.1/35335abbdd4b1a781e6ad47d6aaa2c294d62809f/spring-messaging-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-orm/6.2.1/7fd75b4984d92c3c5e46ee94365cfc79d2daa821/spring-orm-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/6.2.1/def8d3d9bebafc36c19f8407645eddde32454c8a/spring-jdbc-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/6.2.1/5ffde4fee85ff021ad613b9e86a9be893fb52572/spring-tx-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/2.0.10/2990d2957ef0988dd243d06e04d357eace43a522/spring-retry-2.0.10.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/832245e638abf7ec3996b76a42944960abe39e23/kafka_2.13-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.8.1/6278b0f00854c36ed12c8a016152332bc9d1a609/kafka_2.13-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server/3.8.1/9b339e97deeecaf1e7897b4c731d77392b4745de/kafka-server-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator/3.8.1/d372d5bfee64939af353d4246cab795884cb477c/kafka-group-coordinator-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.8.1/3d6662afa30ed98985422d35485962819253040b/kafka-metadata-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.8.1/8f44b9f0da4fa1670d8506d33c06ba7662b37045/kafka-raft-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.8.1/daf35a6f6a1b8da8d2baa754856b12d5ebec7be3/kafka-storage-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.8.1/a522b224b598f30b9637ad2eb3a5db64c14db4a4/kafka-storage-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/483d5f635aa0a751946a6bd80b0b3acbdc6b7169/kafka-server-common-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.8.1/4ad5b80346a131afa11708238441f16026f0ec5c/kafka-server-common-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.8.1/edc3d933c78754476fc5564f4fc2cb3da060a25a/kafka-streams-test-utils-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.8.1/9e054a4e7b88bc80f069e22045826013b7b697ec/kafka-streams-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-group-coordinator-api/3.8.1/9f99f11956404ae64a2b210adaf6d1f3850ab26f/kafka-group-coordinator-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-tools-api/3.8.1/43f69819f286340b5738e562122256b1b2470e99/kafka-tools-api-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/fd79e3aa252c6d818334e9c0bac8166b426e498c/kafka-clients-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.8.1/11d3ffefbc452fc4c5d45f4f6ec368bcab290a95/kafka-clients-3.8.1-test.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/6.2.1/60b7fe91a4ecb88b9ee0d6f228ad0b6a8ac78cb2/spring-test-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.8.4/6638e37b887b5a279044afbdc9928e19f678eb2e/zookeeper-3.8.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.9.0/37fe2217f577b0b68b18e62c4d17a8858ecf9b69/json-path-2.9.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-oxm/6.2.1/5adc025b3f67a1931d0fb938b4fca685178672ac/spring-oxm-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-runtime/4.0.5/ca84c2a7169b5293e232b9d00d1e4e36d4c3914a/jaxb-runtime-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-core/4.0.5/7b4b11ea5542eea4ad55e1080b23be436795b3/jaxb-core-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-core-jakarta/2.2.15/6c85064b85f895b7f0c0819d950995274c0931a4/swagger-core-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/4.0.2/6cd5a999b834b63238005b7144136379dc36cad2/jakarta.xml.bind-api-4.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.5.1/4c11d2808d009132dfbbf947ebf37de6bf266c8e/json-smart-2.5.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.26.3/d26263eb7524252d98e602fc6942996a3195e29/assertj-core-3.26.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.awaitility/awaitility/4.2.2/7336242073ebf83fe034e42b46a403c5501b63c9/awaitility-4.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/jdbc/1.20.4/a02c0d9c5bf18fac3a3e27f2daec1c0137b30b3d/jdbc-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/database-commons/1.20.4/75b6aae47412a70fc8fc7154144cbb57852b2665/database-commons-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.testcontainers/testcontainers/1.20.4/ee2fe3afc9fa6cb2e6a43233998f3633f761692f/testcontainers-1.20.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/junit/junit/4.13.2/8ac9e16d933b6fb43bc7f576336b8f4d7eb5ba12/junit-4.13.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/2.2/3f2bd07716a31c395e2837254f37f21f0f0ab24b/hamcrest-core-2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/5.14.2/f7bf936008d7664e2002c3faf0c02071c8d10e7c/mockito-core-5.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.3/aaa43e0823d2a0e106e8754d6a9c4ab24e05e9bc/jsonassert-1.5.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/3.4.1/3ae5f19bc2b1b30de85b0610ae25818c2e7c295a/spring-data-commons-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/6.2.1/a9384de38fc00751084446ba014a0c4962240244/spring-aop-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/6.2.1/ab57ec03ba6900075bf28e3cd70ccce173205b8d/spring-beans-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/6.2.1/91fcf6b9501705c31c8337e2713fe823bb512b24/spring-expression-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/6.2.1/f42e6b51d9c0c2fcf95df9e5848470d173adc9af/spring-core-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.10.0/1355088731b4ec2107ff7f319f0d7445d916bab/xmlunit-core-2.10.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/3.4.1/5cd01e208b15113c7f88b3ea40e843ea9989f38a/spring-boot-starter-logging-3.4.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/2.1.1/48b9bda22b091b1f48b13af03fe36db3be6e1ae3/jakarta.annotation-api-2.1.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.18.2/7b6ff96adf421f4c6edbd694e797dd8fe434510a/jackson-datatype-jsr310-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.18.2/9ed6d538ebcc66864e114a7040953dce6ab6ea53/jackson-datatype-jdk8-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.18.2/72960cb3277347a748911d100c3302d60e8a616a/jackson-module-parameter-names-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.18.2/84bb533a599a4eaf901aecce5e75d49490a88774/jackson-module-scala_2.13-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.18.2/3e1f715d7cbcc5b79b104115bf8d2a7b0a9b4b22/jackson-dataformat-csv-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.18.2/deef8697b92141fb6caf7aa86966cff4eec9b04f/jackson-databind-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-api/3.4.0/9ef23dcc93693f15e69b64632be096c38e31bc44/docker-java-api-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-models-jakarta/2.2.15/664bc63998b703b3e695a00f502f6091b8c7815f/swagger-models-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.18.2/985d77751ebc7fce5db115a986bc9aa82f973f4a/jackson-annotations-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.18.2/fb64ccac5c27dca8819418eb4e443a9f496d9ee7/jackson-core-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.18.2/d000e13505d1cf564371516fa3d5b8769a779dc9/jackson-dataformat-yaml-2.18.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/2.3/936b36210e27320f920536f695cf1af210c44586/snakeyaml-2.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-commons/1.14.2/69c454dbec59c7842cf59a534b7ec03618d75b91/micrometer-commons-1.14.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.zaxxer/HikariCP/5.1.0/8c96e36c14461fc436bb02b264b96ef3ca5dca8c/HikariCP-5.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.persistence/jakarta.persistence-api/3.1.0/66901fa1c373c6aff65c13791cc11da72060a8d6/jakarta.persistence-api-3.1.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.transaction/jakarta.transaction-api/2.0.1/51a520e3fae406abb84e2e1148e6746ce3f80a1a/jakarta.transaction-api-2.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.6.1.Final/886afbb445b4016a37c8960a7aef6ebd769ce7e5/jboss-logging-3.6.1.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hibernate.common/hibernate-commons-annotations/7.0.3.Final/e183c4be8bb41d12e9f19b374e00c34a0a85f439/hibernate-commons-annotations-7.0.3.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.smallrye/jandex/3.2.0/f17ad860f62a08487b9edabde608f8ac55c62fa7/jandex-3.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.fasterxml/classmate/1.7.0/e98374da1f2143ac8e6e0a95036994bb19137a3/classmate-1.7.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.15.11/f61886478e0f9ee4c21d09574736f0ff45e0a46c/byte-buddy-1.15.11.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.inject/jakarta.inject-api/2.0.1/4c28afe1991a941d7702fe1362c365f0a8641d1e/jakarta.inject-api-2.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.antlr/antlr4-runtime/4.13.0/5a02e48521624faaf5ff4d99afc88b01686af655/antlr4-runtime-4.13.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.5.12/3790d1a62e868f7915776dfb392bd9a29ce8d954/logback-classic-1.5.12.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.9.4/7efe6ccf593e52a2b77f98de52238f15b4a67188/jose4j-0.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.4/e5838ddcf8c1c2b612a7956b9f80423a508675db/scala-logging_2.13-3.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.1.12.1/cb2f351bf4463751201f43bb99865235d5ba07ca/metrics-core-4.1.12.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-transport-zerodep/3.4.0/c4ce6d8695cfdb0027872f99cc20f8f679f8a969/docker-java-transport-zerodep-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.24.3/da1143e2a2531ee1c2d90baa98eb50a28a39d5a7/log4j-to-slf4j-2.24.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/2.0.16/6d57da3e961daac65bcca0dd3def6cd11e48a24a/jul-to-slf4j-2.0.16.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/2.0.16/172931663a09a1fa515567af5fbef00897d3c04/slf4j-api-2.0.16.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.9.22.1/bca243d0af0db4758fbae45c5f4995cb5dabb612/aspectjweaver-1.9.22.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/10.1.34/eef6d430f34b6e393b8d9e40f10db9043732b4e5/tomcat-embed-websocket-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/10.1.34/f610f84be607fbc82e393cc220f0ad45f92afc91/tomcat-embed-core-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/10.1.34/d2b2daca3bc999c62e58ae36b45ba0582530fb25/tomcat-embed-el-10.1.34.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor.netty/reactor-netty-http/1.2.1/5d5fb96ccd9daf98e0c0182da8724f006119cf44/reactor-netty-http-1.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor.netty/reactor-netty-core/1.2.1/9c85dceb89ee078615ff02dd1a9afedbf3c1b14d/reactor-netty-core-1.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.projectreactor/reactor-core/3.7.1/3391286019effb6c179ebbdd2159130dde543c3a/reactor-core-3.7.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns-native-macos/4.1.116.Final/d2449cd6b6b1e3f2c329f685cadf66c10d75ed28/netty-resolver-dns-native-macos-4.1.116.Final-osx-x86_64.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns-classes-macos/4.1.116.Final/216d4b543fb0e046ea819b2a39a5dd794fe9776f/netty-resolver-dns-classes-macos-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver-dns/4.1.116.Final/9f9a00e071e8e63b1aa3db14aacb93c8ecba1ac8/netty-resolver-dns-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-http2/4.1.116.Final/5aefc3cf9ca7df764d0b15b9cb9b6e706a4a55b3/netty-codec-http2-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler-proxy/4.1.116.Final/3e0c3f519305905e41b69dc73064684e31d68357/netty-handler-proxy-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-http/4.1.116.Final/75a0455171848a4188a9f77e9a1e76a036381260/netty-codec-http-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.116.Final/eaef854ef33f3fd3d0ecf927690d8112c710bc05/netty-handler-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-dns/4.1.116.Final/6fa882dde3db7aecc7b7d4882f70f17d9e33b2ad/netty-codec-dns-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec-socks/4.1.116.Final/6d67f4ad92677a29e7a5aae340cb33ea80408c72/netty-codec-socks-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.116.Final/cbe928f601e51a0b5750342b6dad5d35fa12f745/netty-codec-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.116.Final/1c6021a777b96562078d38f62ab8bd7d87627d18/netty-transport-native-epoll-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.116.Final/31ae9fc682c72ec448887b00698e4c8b275ff42d/netty-transport-native-epoll-4.1.116.Final-linux-x86_64.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.116.Final/ce620832453892a92be73f672cd79a36f8c60235/netty-transport-classes-epoll-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.116.Final/1f77d6d8e582ded200e0be1462afbc7563144e12/netty-transport-native-unix-common-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.116.Final/3fa60bbc84222a691bb0686a82308b8e180812b7/netty-transport-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.116.Final/77a86414b9c1e52622e467f1ef7ccb473996742a/netty-buffer-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.116.Final/82fc6bbf59eae483712c8716b8d1f5112b1721a0/netty-resolver-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.116.Final/6871f95af2bc3a98fda34a580baf6cac8cbc2944/netty-common-4.1.116.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/javax.cache/cache-api/1.1.1/c56fb980eb5208bfee29a9a5b9d951aba076bd91/cache-api-1.1.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.reactivex.rxjava3/rxjava/3.1.10/d980c75b983334eeb9a7ea1a252d044f5e6cdf77/rxjava-3.1.10.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.reactivestreams/reactive-streams/1.0.4/3864a1320d97d7b045f729a326e1e077661f31b7/reactive-streams-1.0.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.marshalling/jboss-marshalling-river/2.0.11.Final/d05ee25288b94c93c10e1d8b726305898c4fd5bf/jboss-marshalling-river-2.0.11.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jboss.marshalling/jboss-marshalling/2.0.11.Final/d21ac137e4ab8cff5c7b66445bd03e682d865136/jboss-marshalling-2.0.11.Final.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jodd/jodd-bean/5.1.6/5cbb398dda5c36908b123e98c373d3c1ef72f717/jodd-bean-5.1.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.6-4/ba9e303e0b5e94cdd0017390d7d8c06f47fd61f7/zstd-jni-1.5.6-4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.10.5/ac605269f3598506196e469f1fb0d7ed5c55059e/snappy-java-1.1.10.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.8.4/de7b8a41bbe1ccdfc009de51fa6d160db3ca8025/zookeeper-jute-3.8.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.12.0/e0efa60318229590103e31c69ebdaae56d903644/audience-annotations-0.12.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.5.12/65b1fa25fe8d8e4bdc140e79eb67ac6741f775e2/logback-core-1.5.12.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.14.0/a4c6e1f6c196339473cd2e1b037f0eb97c62755b/commons-io-2.14.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-transaction-coordinator/3.8.1/3c2676a95a416f3d5a59a498c4553914be1d332d/kafka-transaction-coordinator-3.8.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.pcollections/pcollections/4.0.1/59f3bf5fb28c5f5386804dcf129267416b75d7c/pcollections-4.0.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-validator/commons-validator/1.7/76069c915de3787f3ddd8726a56f47a95bfcbb0e/commons-validator-1.7.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.10.0/2464528d14329ff2b44bceff918d6c00300adcf4/scala-collection-compat_2.13-2.10.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.2/5b54f60ccf03e41e24c466462a72475ee918c304/scala-java8-compat_2.13-1.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.14/8e275fefb2a01e178db2cdfebb2181062a790b82/scala-reflect-2.13.14.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.15/ed6f1d58968b16c5f9067d5cac032d952552de58/scala-library-2.13.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.3.0/152ea56b3a72f655d4fd677fc0ef2596c3dd5e6e/opentest4j-1.3.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.eclipse.angus/angus-activation/2.0.2/41f1e0ddd157c856926ed149ab837d110955a9fc/angus-activation-2.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/2.1.3/fa165bd70cda600368eee31555222776a46b881f/jakarta.activation-api-2.1.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.5.1/19b820261eb2e7de7d5bde11d1c06e4501dd7e5f/accessors-smart-2.5.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.15.11/a38b16385e867f59a641330f0362ebe742788ed8/byte-buddy-agent-1.15.11.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.3/1049c09f1de4331e8193e579448d0916d75b7631/objenesis-3.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/6.2.1/a5d662d64470aff0ae51d210147bb6ede31a8ea3/spring-jcl-6.2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-compress/1.24.0/b4b1b5a3d9573b2970fddab236102c0a4d27d35e/commons-compress-1.24.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.rnorth.duct-tape/duct-tape/1.0.8/92edc22a9ab2f3e17c9bf700aaee377d50e8b530/duct-tape-1.0.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.2.2/7959933ebcc0f05b2eaa5af0a0c8689fa257b15c/HdrHistogram-2.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.latencyutils/LatencyUtils/2.0.3/769c0b82cb2421c8256300e907298a9410a2a3d3/LatencyUtils-2.0.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jodd/jodd-core/5.1.6/7e3904364d0891b58a958c47336d35518084ddec/jodd-core-5.1.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/7.9.2/6409b667493149191b09fe1fce94bada6096a3e9/rocksdbjni-7.9.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.ben-manes.caffeine/caffeine/3.1.8/24795585df8afaf70a2cd534786904ea5889c047/caffeine-3.1.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.9.4/d52b9abcd97f38c81342bb7e7ae1eee9b73cba51/commons-beanutils-1.9.4.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/2.1/73a8001e7a54a255eef0f03521ec1805dc738ca0/commons-digester-2.1.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.2/8ad72fe39fa8c91eaaf12aadb21e0c3661fe26d5/commons-collections-3.2.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.6/aa205cf0a06dbd8e04ece91c0b37c3f5d567546a/asm-9.6.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.jetbrains/annotations/17.0.0/8ceead41f4e71821919dbdb7a9847608f1a938cb/annotations-17.0.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.github.docker-java/docker-java-transport/3.4.0/c058705684d782effc4b2edfdef1a87544ba4af8/docker-java-transport-3.4.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/5.13.0/1200e7ebeedbe0d10062093f32925a912020e747/jna-5.13.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.24.3/b02c125db8b6d295adf72ae6e71af5d83bce2370/log4j-api-2.24.3.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/txw2/4.0.5/f36a4ef12120a9bb06d766d6a0e54b144fd7ed98/txw2-4.0.5.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.sun.istack/istack-commons-runtime/4.1.2/18ec117c85f3ba0ac65409136afa8e42bc74e739/istack-commons-runtime-4.1.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-lang3/3.17.0/b17d2136f0460dcc0d2016ceefca8723bdf4ee70/commons-lang3-3.17.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/io.swagger.core.v3/swagger-annotations-jakarta/2.2.15/951eda18dbce0397347056aceff38d02710ec866/swagger-annotations-jakarta-2.2.15.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/jakarta.validation/jakarta.validation-api/3.0.2/92b6631659ba35ca09e44874d3eb936edfeee532/jakarta.validation-api-3.0.2.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.37.0/ba74746d38026581c12166e164bb3c15e90cc4ea/checker-qual-3.37.0.jar:/Users/seongdo/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.21.1/6d9b10773b5237df178a7b3c1b4208df7d0e7f94/error_prone_annotations-2.21.1.jar
2025-02-16 15:17:55.179 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/Users/seongdo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Mac OS X
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=aarch64
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=15.3
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=seongdo
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/Users/seongdo
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/Users/seongdo/Desktop/hhplus_week3/server-java
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=44MB
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=512MB
2025-02-16 15:17:55.180 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=104MB
2025-02-16 15:17:55.182 [Test worker]  INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=127.0.0.1:50088 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@62b3871
2025-02-16 15:17:55.183 [Test worker]  INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
2025-02-16 15:17:55.188 [Test worker]  INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=false
2025-02-16 15:17:55.189 [Test worker]  INFO kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Waiting until connected.
2025-02-16 15:17:55.189 [Test worker-SendThread(127.0.0.1:50088)]  INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server /127.0.0.1:50088.
2025-02-16 15:17:55.189 [Test worker-SendThread(127.0.0.1:50088)]  INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /127.0.0.1:50089, server: /127.0.0.1:50088
2025-02-16 15:17:55.195 [SyncThread:0]  INFO org.apache.zookeeper.server.persistence.FileTxnLog - Creating new log file: log.1
2025-02-16 15:17:55.198 [SyncThread:0]  INFO org.apache.zookeeper.audit.ZKAuditProvider - ZooKeeper audit is disabled.
2025-02-16 15:17:55.200 [Test worker-SendThread(127.0.0.1:50088)]  INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server /127.0.0.1:50088, session id = 0x100003e249e0000, negotiated timeout = 16000
2025-02-16 15:17:55.201 [Test worker]  INFO kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Connected.
2025-02-16 15:17:55.287 [Test worker]  INFO kafka.server.KafkaServer - Cluster ID = m7CWMxGoRDCRewfvPt7Wfg
2025-02-16 15:17:55.290 [Test worker]  INFO org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig - RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-02-16 15:17:55.314 [Test worker]  INFO org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig - RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-02-16 15:17:55.318 [Test worker]  INFO kafka.server.KafkaConfig - KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 2
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 2
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = true
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:50088
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

2025-02-16 15:17:55.320 [Test worker]  INFO org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig - RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false

2025-02-16 15:17:55.340 [ThrottledChannelReaper-Fetch]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Starting
2025-02-16 15:17:55.340 [ThrottledChannelReaper-Produce]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Starting
2025-02-16 15:17:55.341 [ThrottledChannelReaper-Request]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Starting
2025-02-16 15:17:55.342 [ThrottledChannelReaper-ControllerMutation]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Starting
2025-02-16 15:17:55.346 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] Rewriting /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/meta.properties
2025-02-16 15:17:55.380 [Test worker]  INFO kafka.log.LogManager - Loading logs from log dirs ArrayBuffer(/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164)
2025-02-16 15:17:55.382 [Test worker]  INFO kafka.log.LogManager - No logs found to be loaded in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164
2025-02-16 15:17:55.387 [Test worker]  INFO kafka.log.LogManager - Loaded 0 logs in 7ms
2025-02-16 15:17:55.388 [Test worker]  INFO kafka.log.LogManager - Starting log cleanup with a period of 300000 ms.
2025-02-16 15:17:55.388 [Test worker]  INFO kafka.log.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
2025-02-16 15:17:55.397 [Test worker]  INFO kafka.log.LogCleaner - Starting the log cleaner
2025-02-16 15:17:55.403 [kafka-log-cleaner-thread-0]  INFO kafka.log.LogCleaner$CleanerThread - [kafka-log-cleaner-thread-0]: Starting
2025-02-16 15:17:55.414 [feature-zk-node-event-process-thread]  INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Starting
2025-02-16 15:17:55.420 [feature-zk-node-event-process-thread]  INFO kafka.server.FinalizedFeatureChangeListener - Feature ZK node at path: /feature does not exist
2025-02-16 15:17:55.434 [zk-broker-0-to-controller-forwarding-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-forwarding-channel-manager]: Starting
2025-02-16 15:17:55.687 [Test worker]  INFO kafka.network.ConnectionQuotas - Updated connection-accept-rate max connection creation rate to 2147483647
2025-02-16 15:17:55.692 [Test worker]  INFO kafka.network.DataPlaneAcceptor - Awaiting socket connections on localhost:50090.
2025-02-16 15:17:55.692 [Test worker]  INFO kafka.network.DataPlaneAcceptor - Opened wildcard endpoint localhost:50090
2025-02-16 15:17:55.701 [Test worker]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2025-02-16 15:17:55.704 [zk-broker-0-to-controller-alter-partition-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting
2025-02-16 15:17:55.723 [ExpirationReaper-0-Produce]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Starting
2025-02-16 15:17:55.723 [ExpirationReaper-0-Fetch]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Starting
2025-02-16 15:17:55.724 [ExpirationReaper-0-DeleteRecords]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Starting
2025-02-16 15:17:55.724 [ExpirationReaper-0-ElectLeader]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Starting
2025-02-16 15:17:55.725 [ExpirationReaper-0-RemoteFetch]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-RemoteFetch]: Starting
2025-02-16 15:17:55.735 [LogDirFailureHandler]  INFO kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Starting
2025-02-16 15:17:55.736 [AddPartitionsToTxnSenderThread-0]  INFO kafka.server.AddPartitionsToTxnManager - [AddPartitionsToTxnSenderThread-0]: Starting
2025-02-16 15:17:55.754 [Test worker]  INFO kafka.zk.KafkaZkClient - Creating /brokers/ids/0 (is it secure? false)
2025-02-16 15:17:55.763 [Test worker]  INFO kafka.zk.KafkaZkClient - Stat of the created znode at /brokers/ids/0 is: 25,25,1739719075760,1739719075760,1,0,0,72057860940234752,204,0,25

2025-02-16 15:17:55.764 [Test worker]  INFO kafka.zk.KafkaZkClient - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:50090, czxid (broker epoch): 25
2025-02-16 15:17:55.793 [controller-event-thread]  INFO kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Starting
2025-02-16 15:17:55.800 [ExpirationReaper-0-topic]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Starting
2025-02-16 15:17:55.802 [controller-event-thread]  INFO kafka.zk.KafkaZkClient - Successfully created /controller_epoch with initial epoch 0
2025-02-16 15:17:55.804 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2025-02-16 15:17:55.806 [ExpirationReaper-0-Heartbeat]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Starting
2025-02-16 15:17:55.806 [ExpirationReaper-0-Rebalance]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Starting
2025-02-16 15:17:55.807 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
2025-02-16 15:17:55.808 [Test worker-EventThread]  INFO kafka.server.FinalizedFeatureChangeListener - Feature ZK node created at path: /feature
2025-02-16 15:17:55.821 [Test worker]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Starting up.
2025-02-16 15:17:55.823 [Test worker]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Startup complete.
2025-02-16 15:17:55.826 [feature-zk-node-event-process-thread]  INFO kafka.server.metadata.ZkMetadataCache - [MetadataCache brokerId=0] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0).
2025-02-16 15:17:55.826 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Registering handlers
2025-02-16 15:17:55.828 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Deleting log dir event notifications
2025-02-16 15:17:55.829 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Deleting isr change notifications
2025-02-16 15:17:55.829 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Initializing controller context
2025-02-16 15:17:55.834 [Test worker]  INFO kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Starting up.
2025-02-16 15:17:55.835 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)
2025-02-16 15:17:55.835 [TxnMarkerSenderThread-0]  INFO kafka.coordinator.transaction.TransactionMarkerChannelManager - [TxnMarkerSenderThread-0]: Starting
2025-02-16 15:17:55.836 [Test worker]  INFO kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Startup complete.
2025-02-16 15:17:55.841 [Controller-0-to-broker-0-send-thread]  INFO kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Starting
2025-02-16 15:17:55.842 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Currently active brokers in the cluster: Set(0)
2025-02-16 15:17:55.842 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2025-02-16 15:17:55.842 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Current list of topics in the cluster: HashSet()
2025-02-16 15:17:55.842 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Fetching topic deletions in progress
2025-02-16 15:17:55.843 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] List of topics to be deleted: 
2025-02-16 15:17:55.843 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] List of topics ineligible for deletion: 
2025-02-16 15:17:55.843 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Initializing topic deletion manager
2025-02-16 15:17:55.844 [controller-event-thread]  INFO kafka.controller.TopicDeletionManager - [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2025-02-16 15:17:55.845 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Sending update metadata request
2025-02-16 15:17:55.846 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2025-02-16 15:17:55.850 [controller-event-thread]  INFO kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Initializing replica state
2025-02-16 15:17:55.850 [controller-event-thread]  INFO kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2025-02-16 15:17:55.853 [controller-event-thread]  INFO kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2025-02-16 15:17:55.853 [controller-event-thread]  INFO kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Initializing partition state
2025-02-16 15:17:55.853 [controller-event-thread]  INFO kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Triggering online partition state changes
2025-02-16 15:17:55.864 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Ready to serve as the new controller with epoch 1
2025-02-16 15:17:55.866 [Controller-0-to-broker-0-send-thread]  INFO kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Controller 0 connected to localhost:50090 (id: 0 rack: null) for sending state change requests
2025-02-16 15:17:55.869 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Partitions undergoing preferred replica election: 
2025-02-16 15:17:55.869 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Partitions that completed preferred replica election: 
2025-02-16 15:17:55.869 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2025-02-16 15:17:55.869 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Resuming preferred replica election for partitions: 
2025-02-16 15:17:55.870 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2025-02-16 15:17:55.886 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Starting the controller scheduler
2025-02-16 15:17:55.898 [ExpirationReaper-0-AlterAcls]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Starting
2025-02-16 15:17:55.916 [/config/changes-event-process-thread]  INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Starting
2025-02-16 15:17:55.919 [Test worker]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
2025-02-16 15:17:55.921 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] Start processing authorizer futures
2025-02-16 15:17:55.921 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] End processing authorizer futures
2025-02-16 15:17:55.921 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] Start processing enable request processing future
2025-02-16 15:17:55.921 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] End processing enable request processing future
2025-02-16 15:17:55.922 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 15:17:55.922 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 15:17:55.922 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739719075921
2025-02-16 15:17:55.922 [Test worker]  INFO kafka.server.KafkaServer - [KafkaServer id=0] started
2025-02-16 15:17:55.929 [Test worker]  INFO org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [127.0.0.1:50090]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-02-16 15:17:55.940 [zk-broker-0-to-controller-forwarding-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node localhost:50090 (id: 0 rack: null)
2025-02-16 15:17:55.952 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 15:17:55.952 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 15:17:55.952 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739719075952
2025-02-16 15:17:55.978 [data-plane-kafka-request-handler-7]  INFO kafka.zk.AdminZkClient - Creating topic test-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-02-16 15:17:55.989 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New topics: [Set(test-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test-topic,Some(Z_VKG5v5SNCUtMV_T4XGhQ),Map(test-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-02-16 15:17:55.989 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New partition creation callback for test-topic-0
2025-02-16 15:17:55.990 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition test-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:17:55.990 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:17:55.993 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:17:56.001 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition test-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:17:56.002 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-02-16 15:17:56.003 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-02-16 15:17:56.003 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:17:56.005 [data-plane-kafka-request-handler-3]  INFO state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2025-02-16 15:17:56.006 [zk-broker-0-to-controller-alter-partition-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node localhost:50090 (id: 0 rack: null)
2025-02-16 15:17:56.017 [data-plane-kafka-request-handler-3]  INFO kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-topic-0)
2025-02-16 15:17:56.017 [data-plane-kafka-request-handler-3]  INFO state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-02-16 15:17:56.050 [data-plane-kafka-request-handler-3]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=test-topic-0, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:17:56.057 [data-plane-kafka-request-handler-3]  INFO kafka.log.LogManager - Created log for partition test-topic-0 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/test-topic-0 with properties {}
2025-02-16 15:17:56.058 [data-plane-kafka-request-handler-3]  INFO kafka.cluster.Partition - [Partition test-topic-0 broker=0] No checkpointed highwatermark is found for partition test-topic-0
2025-02-16 15:17:56.059 [data-plane-kafka-request-handler-3]  INFO kafka.cluster.Partition - [Partition test-topic-0 broker=0] Log loaded for partition test-topic-0 with initial high watermark 0
2025-02-16 15:17:56.061 [data-plane-kafka-request-handler-3]  INFO state.change.logger - [Broker id=0] Leader test-topic-0 with topic id Some(Z_VKG5v5SNCUtMV_T4XGhQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:17:56.069 [data-plane-kafka-request-handler-3]  INFO state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 64ms correlationId 1 from controller 0 for 1 partitions
2025-02-16 15:17:56.074 [data-plane-kafka-request-handler-2]  INFO state.change.logger - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2025-02-16 15:17:56.084 [kafka-admin-client-thread | adminclient-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
2025-02-16 15:17:56.087 [kafka-admin-client-thread | adminclient-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 15:17:56.087 [kafka-admin-client-thread | adminclient-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 15:17:56.087 [kafka-admin-client-thread | adminclient-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 15:17:56.103 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaProducerIntegrationTest - Starting KafkaProducerIntegrationTest using Java 17.0.14 with PID 1819 (started by seongdo in /Users/seongdo/Desktop/hhplus_week3/server-java)
2025-02-16 15:17:56.103 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaProducerIntegrationTest - The following 1 profile is active: "local"
2025-02-16 15:17:56.534 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 15:17:56.534 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 15:17:56.555 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 JPA repository interfaces.
2025-02-16 15:17:56.557 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 15:17:56.557 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 15:17:56.746 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 187 ms. Found 9 JPA repository interfaces.
2025-02-16 15:17:57.056 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 15:17:57.057 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.CouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.UserCouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderItemJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.payment.infrastructure.PaymentJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.product.infrastructure.ProductJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.stats.infrastructure.SalesStatsJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.075 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserPointJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 15:17:57.075 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 10 ms. Found 0 Redis repository interfaces.
2025-02-16 15:17:57.216 [Test worker]  INFO org.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2025-02-16 15:17:57.218 [Test worker]  INFO org.testcontainers.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2025-02-16 15:17:57.223 [Test worker]  INFO org.testcontainers.DockerClientFactory - Testcontainers version: 1.20.4
2025-02-16 15:17:57.399 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2025-02-16 15:17:57.587 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2025-02-16 15:17:57.588 [Test worker]  INFO org.testcontainers.DockerClientFactory - Docker host IP address is localhost
2025-02-16 15:17:57.602 [Test worker]  INFO org.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 27.4.0
  API Version: 1.47
  Operating System: Docker Desktop
  Total Memory: 7837 MB
  Labels: 
    com.docker.desktop.address=unix:///Users/seongdo/Library/Containers/com.docker.docker/Data/docker-cli.sock
2025-02-16 15:17:57.621 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Creating container for image: testcontainers/ryuk:0.11.0
2025-02-16 15:17:57.730 [Test worker]  INFO org.testcontainers.utility.RegistryAuthLocator - Credential helper/store (docker-credential-desktop) does not have credentials for https://index.docker.io/v1/
2025-02-16 15:17:57.806 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 is starting: 4d7d83953d5253791591f71a46f8d4d7d786efa40c145b91dc60ed2a87f921e0
2025-02-16 15:17:58.054 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 started in PT0.432891S
2025-02-16 15:17:58.058 [Test worker]  INFO org.testcontainers.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2025-02-16 15:17:58.058 [Test worker]  INFO org.testcontainers.DockerClientFactory - Checking the system...
2025-02-16 15:17:58.058 [Test worker]  INFO org.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2025-02-16 15:17:58.061 [Test worker]  INFO tc.mysql:8.0 - Creating container for image: mysql:8.0
2025-02-16 15:17:58.135 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 is starting: a2c379fb40a96fe427e23f6ea089030d18d7657fc21f5c35af4981b66dd94402
2025-02-16 15:17:58.300 [Test worker]  INFO tc.mysql:8.0 - Waiting for database connection to become available at jdbc:mysql://localhost:50096/hhplus using query 'SELECT 1'
2025-02-16 15:18:00.892 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Processing automatic preferred replica leader election
2025-02-16 15:18:05.182 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 started in PT7.120633S
2025-02-16 15:18:05.182 [Test worker]  INFO tc.mysql:8.0 - Container is started (JDBC URL: jdbc:mysql://localhost:50096/hhplus)
2025-02-16 15:18:05.183 [Test worker]  INFO tc.redis:7.4.2 - Creating container for image: redis:7.4.2
2025-02-16 15:18:05.221 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 is starting: 455bb10d5ba72bd0871eb279720940e0a14fb4184f2c89f8e3f0f9a30a307d46
2025-02-16 15:18:05.477 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 started in PT0.294422S
2025-02-16 15:18:05.878 [Test worker]  INFO org.hibernate.jpa.internal.util.LogHelper - HHH000204: Processing PersistenceUnitInfo [name: default]
2025-02-16 15:18:05.928 [Test worker]  INFO org.hibernate.Version - HHH000412: Hibernate ORM core version 6.6.4.Final
2025-02-16 15:18:05.953 [Test worker]  INFO org.hibernate.cache.internal.RegionFactoryInitiator - HHH000026: Second-level cache disabled
2025-02-16 15:18:06.152 [Test worker]  INFO org.springframework.orm.jpa.persistenceunit.SpringPersistenceUnitInfo - No LoadTimeWeaver setup: ignoring JPA class transformer
2025-02-16 15:18:06.172 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Starting...
2025-02-16 15:18:06.265 [Test worker]  INFO com.zaxxer.hikari.pool.HikariPool - HangHaePlusDataSource - Added connection com.mysql.cj.jdbc.ConnectionImpl@6ed2f215
2025-02-16 15:18:06.266 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Start completed.
2025-02-16 15:18:06.331 [Test worker]  INFO org.hibernate.orm.connections.pooling - HHH10001005: Database info:
	Database JDBC URL [Connecting through datasource 'HikariDataSource (HangHaePlusDataSource)']
	Database driver: undefined/unknown
	Database version: 8.0.41
	Autocommit mode: undefined/unknown
	Isolation level: undefined/unknown
	Minimum pool size: undefined/unknown
	Maximum pool size: undefined/unknown
2025-02-16 15:18:07.059 [Test worker]  INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator - HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-02-16 15:18:07.061 [Test worker]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 15:18:07.313 [Test worker]  INFO org.springframework.data.jpa.repository.query.QueryEnhancerFactory - Hibernate is in classpath; If applicable, HQL parser will be used.
2025-02-16 15:18:07.746 [Test worker] ERROR io.netty.resolver.dns.DnsServerAddressStreamProviders - Unable to load io.netty.resolver.dns.macos.MacOSDnsServerAddressStreamProvider, fallback to system defaults. This may result in incorrect DNS resolutions on MacOS. Check whether you have a dependency on 'io.netty:netty-resolver-dns-native-macos'. Use DEBUG level to see the full stack: java.lang.UnsatisfiedLinkError: failed to load the required native library
2025-02-16 15:18:08.084 [Test worker]  INFO org.redisson.Version - Redisson 3.17.6
2025-02-16 15:18:08.166 [redisson-netty-4-6]  INFO org.redisson.connection.pool.MasterPubSubConnectionPool - 1 connections initialized for localhost/127.0.0.1:50158
2025-02-16 15:18:08.195 [redisson-netty-4-19]  INFO org.redisson.connection.pool.MasterConnectionPool - 24 connections initialized for localhost/127.0.0.1:50158
2025-02-16 15:18:08.751 [Test worker]  INFO org.springframework.validation.beanvalidation.OptionalValidatorFactoryBean - Failed to set up a Bean Validation provider: jakarta.validation.NoProviderFoundException: Unable to create a Configuration, because no Jakarta Bean Validation provider could be found. Add a provider like Hibernate Validator (RI) to your classpath.
2025-02-16 15:18:09.190 [Test worker]  INFO org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver - Exposing 1 endpoint beneath base path '/actuator'
2025-02-16 15:18:09.307 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:50090]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spring-kafka-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spring-kafka-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 15:18:09.338 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 15:18:09.371 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 15:18:09.371 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 15:18:09.371 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739719089371
2025-02-16 15:18:09.377 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Subscribed to topic(s): sample-topic
2025-02-16 15:18:09.393 [data-plane-kafka-request-handler-2]  INFO kafka.zk.AdminZkClient - Creating topic sample-topic with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2025-02-16 15:18:09.395 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaProducerIntegrationTest - Started KafkaProducerIntegrationTest in 14.915 seconds (process running for 20.985)
2025-02-16 15:18:09.397 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New topics: [Set(sample-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(sample-topic,Some(XBk5l7VnRAOLYWwPaSYVHg),Map(sample-topic-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-02-16 15:18:09.397 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New partition creation callback for sample-topic-0
2025-02-16 15:18:09.397 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition sample-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.397 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.397 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.398 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  WARN org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Error while fetching metadata with correlation id 2 : {sample-topic=LEADER_NOT_AVAILABLE}
2025-02-16 15:18:09.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Cluster ID: m7CWMxGoRDCRewfvPt7Wfg
2025-02-16 15:18:09.400 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition sample-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.400 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2025-02-16 15:18:09.400 [data-plane-kafka-request-handler-1]  INFO kafka.zk.AdminZkClient - Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2025-02-16 15:18:09.400 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2025-02-16 15:18:09.401 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.401 [data-plane-kafka-request-handler-4]  INFO state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions
2025-02-16 15:18:09.403 [data-plane-kafka-request-handler-4]  INFO kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sample-topic-0)
2025-02-16 15:18:09.403 [data-plane-kafka-request-handler-4]  INFO state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2025-02-16 15:18:09.406 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(F1pD4TmBTmev2Fy53aBEMQ),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2025-02-16 15:18:09.406 [data-plane-kafka-request-handler-4]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=sample-topic-0, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.406 [data-plane-kafka-request-handler-4]  INFO kafka.log.LogManager - Created log for partition sample-topic-0 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/sample-topic-0 with properties {}
2025-02-16 15:18:09.406 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2025-02-16 15:18:09.407 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.407 [data-plane-kafka-request-handler-4]  INFO kafka.cluster.Partition - [Partition sample-topic-0 broker=0] No checkpointed highwatermark is found for partition sample-topic-0
2025-02-16 15:18:09.407 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.407 [data-plane-kafka-request-handler-4]  INFO kafka.cluster.Partition - [Partition sample-topic-0 broker=0] Log loaded for partition sample-topic-0 with initial high watermark 0
2025-02-16 15:18:09.407 [data-plane-kafka-request-handler-4]  INFO state.change.logger - [Broker id=0] Leader sample-topic-0 with topic id Some(XBk5l7VnRAOLYWwPaSYVHg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.407 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.407 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.407 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2025-02-16 15:18:09.410 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.410 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.413 [data-plane-kafka-request-handler-4]  INFO state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 12ms correlationId 3 from controller 0 for 1 partitions
2025-02-16 15:18:09.414 [data-plane-kafka-request-handler-5]  INFO state.change.logger - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)
2025-02-16 15:18:09.415 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2025-02-16 15:18:09.416 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2025-02-16 15:18:09.416 [controller-event-thread]  INFO state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2025-02-16 15:18:09.416 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions
2025-02-16 15:18:09.417 [data-plane-kafka-request-handler-0]  INFO kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2025-02-16 15:18:09.417 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2025-02-16 15:18:09.420 [data-plane-kafka-request-handler-0]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-3, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.420 [data-plane-kafka-request-handler-0]  INFO kafka.log.LogManager - Created log for partition __consumer_offsets-3 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-02-16 15:18:09.420 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2025-02-16 15:18:09.420 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2025-02-16 15:18:09.420 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Leader __consumer_offsets-3 with topic id Some(F1pD4TmBTmev2Fy53aBEMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.426 [data-plane-kafka-request-handler-0]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-2, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.426 [data-plane-kafka-request-handler-0]  INFO kafka.log.LogManager - Created log for partition __consumer_offsets-2 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-02-16 15:18:09.426 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2025-02-16 15:18:09.426 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2025-02-16 15:18:09.426 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Leader __consumer_offsets-2 with topic id Some(F1pD4TmBTmev2Fy53aBEMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.434 [data-plane-kafka-request-handler-0]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-4, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.434 [data-plane-kafka-request-handler-0]  INFO kafka.log.LogManager - Created log for partition __consumer_offsets-4 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-02-16 15:18:09.434 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2025-02-16 15:18:09.434 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2025-02-16 15:18:09.435 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Leader __consumer_offsets-4 with topic id Some(F1pD4TmBTmev2Fy53aBEMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.441 [data-plane-kafka-request-handler-0]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-1, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.442 [data-plane-kafka-request-handler-0]  INFO kafka.log.LogManager - Created log for partition __consumer_offsets-1 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-02-16 15:18:09.442 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2025-02-16 15:18:09.442 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2025-02-16 15:18:09.442 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Leader __consumer_offsets-1 with topic id Some(F1pD4TmBTmev2Fy53aBEMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.449 [data-plane-kafka-request-handler-0]  INFO kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-0, dir=/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164] Loading producer state till offset 0 with message format version 2
2025-02-16 15:18:09.450 [data-plane-kafka-request-handler-0]  INFO kafka.log.LogManager - Created log for partition __consumer_offsets-0 in /var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
2025-02-16 15:18:09.450 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2025-02-16 15:18:09.450 [data-plane-kafka-request-handler-0]  INFO kafka.cluster.Partition - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2025-02-16 15:18:09.450 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Leader __consumer_offsets-0 with topic id Some(F1pD4TmBTmev2Fy53aBEMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.
2025-02-16 15:18:09.454 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2025-02-16 15:18:09.455 [data-plane-kafka-request-handler-0]  INFO state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 39ms correlationId 5 from controller 0 for 5 partitions
2025-02-16 15:18:09.457 [data-plane-kafka-request-handler-6]  INFO state.change.logger - [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
2025-02-16 15:18:09.457 [group-metadata-manager-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 2 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler.
2025-02-16 15:18:09.457 [group-metadata-manager-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-02-16 15:18:09.457 [group-metadata-manager-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-02-16 15:18:09.457 [group-metadata-manager-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-02-16 15:18:09.457 [group-metadata-manager-0]  INFO kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2025-02-16 15:18:09.524 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Discovered group coordinator localhost:50090 (id: 2147483647 rack: null)
2025-02-16 15:18:09.525 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] (Re-)joining group
2025-02-16 15:18:09.543 [data-plane-kafka-request-handler-4]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Dynamic member with unknown member id joins group spring-kafka-group in Empty state. Created a new member id consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658 and request the member to rejoin with this id.
2025-02-16 15:18:09.546 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: need to re-join with the given member-id: consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658
2025-02-16 15:18:09.547 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] (Re-)joining group
2025-02-16 15:18:09.555 [data-plane-kafka-request-handler-5]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Preparing to rebalance group spring-kafka-group in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658 with group instance id None; client reason: need to re-join with the given member-id: consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658)
2025-02-16 15:18:09.559 [executor-Rebalance]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Stabilized group spring-kafka-group generation 1 (__consumer_offsets-0) with 1 members
2025-02-16 15:18:09.560 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658', protocol='range'}
2025-02-16 15:18:09.564 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Finished assignment for group at generation 1: {consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658=Assignment(partitions=[sample-topic-0])}
2025-02-16 15:18:09.569 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Assignment received from leader consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658 for group spring-kafka-group for generation 1. The group has 1 members, 0 of which are static.
2025-02-16 15:18:09.598 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658', protocol='range'}
2025-02-16 15:18:09.599 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Notifying assignor about the new Assignment(partitions=[sample-topic-0])
2025-02-16 15:18:09.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Adding newly assigned partitions: sample-topic-0
2025-02-16 15:18:09.606 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Found no committed offset for partition sample-topic-0
2025-02-16 15:18:09.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting offset for partition sample-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50090 (id: 0 rack: null)], epoch=0}}.
2025-02-16 15:18:09.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: partitions assigned: [sample-topic-0]
2025-02-16 15:18:09.731 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 10
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:50090]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-testGroup-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testGroup
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 60000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 15:18:09.732 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 15:18:09.736 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 15:18:09.736 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 15:18:09.736 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739719089736
2025-02-16 15:18:09.736 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Subscribed to topic(s): test-topic
2025-02-16 15:18:09.740 [Test worker]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Cluster ID: m7CWMxGoRDCRewfvPt7Wfg
2025-02-16 15:18:09.740 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Discovered group coordinator localhost:50090 (id: 2147483647 rack: null)
2025-02-16 15:18:09.742 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] (Re-)joining group
2025-02-16 15:18:09.743 [data-plane-kafka-request-handler-0]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Dynamic member with unknown member id joins group testGroup in Empty state. Created a new member id consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded and request the member to rejoin with this id.
2025-02-16 15:18:09.743 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Request joining group due to: need to re-join with the given member-id: consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded
2025-02-16 15:18:09.743 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] (Re-)joining group
2025-02-16 15:18:09.744 [data-plane-kafka-request-handler-6]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Preparing to rebalance group testGroup in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded with group instance id None; client reason: need to re-join with the given member-id: consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded)
2025-02-16 15:18:09.744 [executor-Rebalance]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Stabilized group testGroup generation 1 (__consumer_offsets-4) with 1 members
2025-02-16 15:18:09.745 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Successfully joined group with generation Generation{generationId=1, memberId='consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded', protocol='range'}
2025-02-16 15:18:09.745 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Finished assignment for group at generation 1: {consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded=Assignment(partitions=[test-topic-0])}
2025-02-16 15:18:09.745 [data-plane-kafka-request-handler-7]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Assignment received from leader consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded for group testGroup for generation 1. The group has 1 members, 0 of which are static.
2025-02-16 15:18:09.746 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Successfully synced group in generation Generation{generationId=1, memberId='consumer-testGroup-2-6e361a72-b81d-4fbd-8e56-f43224a67ded', protocol='range'}
2025-02-16 15:18:09.746 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-02-16 15:18:09.746 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Adding newly assigned partitions: test-topic-0
2025-02-16 15:18:09.747 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Found no committed offset for partition test-topic-0
2025-02-16 15:18:09.749 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Resetting offset for partition test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50090 (id: 0 rack: null)], epoch=0}}.
2025-02-16 15:18:09.837 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Seeking to earliest offset of partition test-topic-0
2025-02-16 15:18:09.846 [Test worker]  INFO org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:50090]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = hhplus-producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-02-16 15:18:09.846 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 15:18:09.853 [Test worker]  INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=hhplus-producer-1] Instantiated an idempotent producer.
2025-02-16 15:18:09.862 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 15:18:09.862 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 15:18:09.862 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739719089862
2025-02-16 15:18:09.865 [kafka-producer-network-thread | hhplus-producer-1]  INFO org.apache.kafka.clients.Metadata - [Producer clientId=hhplus-producer-1] Cluster ID: m7CWMxGoRDCRewfvPt7Wfg
2025-02-16 15:18:09.872 [controller-event-thread]  INFO kafka.controller.KafkaController - [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=0, firstProducerId=0, size=1000) by writing to Zk with path version 1
2025-02-16 15:18:09.973 [kafka-producer-network-thread | hhplus-producer-1]  INFO org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=hhplus-producer-1] ProducerId set to 0 with epoch 0
2025-02-16 15:18:09.998 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Resetting offset for partition test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:50090 (id: 0 rack: null)], epoch=0}}.
2025-02-16 15:18:10.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Revoke previously assigned partitions sample-topic-0
2025-02-16 15:18:10.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: partitions revoked: [sample-topic-0]
2025-02-16 15:18:10.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Member consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658 sending LeaveGroup request to coordinator localhost:50090 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-02-16 15:18:10.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 15:18:10.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 15:18:10.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Unsubscribed all topics or patterns and assigned partitions
2025-02-16 15:18:10.312 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 15:18:10.312 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 15:18:10.312 [data-plane-kafka-request-handler-6]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Preparing to rebalance group spring-kafka-group in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Removing member consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658 on LeaveGroup; client reason: the consumer unsubscribed from all topics)
2025-02-16 15:18:10.313 [data-plane-kafka-request-handler-6]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Group spring-kafka-group with generation 2 is now empty (__consumer_offsets-0)
2025-02-16 15:18:10.314 [data-plane-kafka-request-handler-6]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-spring-kafka-group-1-9d73e973-c80a-45bf-877c-8bdc22419658, groupInstanceId=None, clientId=consumer-spring-kafka-group-1, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group spring-kafka-group through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics
2025-02-16 15:18:10.638 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 15:18:10.638 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 15:18:10.638 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 15:18:10.638 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 15:18:10.641 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spring-kafka-group-1 unregistered
2025-02-16 15:18:10.650 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: Consumer stopped
2025-02-16 15:18:12.732 [SpringApplicationShutdownHook]  INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=hhplus-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-02-16 15:18:12.737 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 15:18:12.737 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 15:18:12.737 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 15:18:12.737 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 15:18:12.738 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for hhplus-producer-1 unregistered
2025-02-16 15:18:13.034 [SpringApplicationShutdownHook]  INFO kafka.server.KafkaServer - [KafkaServer id=0] shutting down
2025-02-16 15:18:13.035 [SpringApplicationShutdownHook]  INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Shutting down
2025-02-16 15:18:13.035 [/config/changes-event-process-thread]  INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Stopped
2025-02-16 15:18:13.035 [SpringApplicationShutdownHook]  INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Shutdown completed
2025-02-16 15:18:13.035 [SpringApplicationShutdownHook]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2025-02-16 15:18:13.045 [zk-broker-0-to-controller-forwarding-channel-manager]  INFO org.apache.kafka.clients.NetworkClient - [NodeToControllerChannelManager id=0 name=forwarding] Node 0 disconnected.
2025-02-16 15:18:13.047 [SpringApplicationShutdownHook]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2025-02-16 15:18:13.047 [SpringApplicationShutdownHook]  INFO kafka.server.KafkaRequestHandlerPool - [data-plane Kafka Request Handler on Broker 0], shutting down
2025-02-16 15:18:13.048 [SpringApplicationShutdownHook]  INFO kafka.server.KafkaRequestHandlerPool - [data-plane Kafka Request Handler on Broker 0], shut down completely
2025-02-16 15:18:13.048 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Shutting down
2025-02-16 15:18:13.049 [ExpirationReaper-0-AlterAcls]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Stopped
2025-02-16 15:18:13.049 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2025-02-16 15:18:13.049 [SpringApplicationShutdownHook]  INFO kafka.server.KafkaApis - [KafkaApi-0] Shutdown complete.
2025-02-16 15:18:13.050 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Shutting down
2025-02-16 15:18:13.050 [ExpirationReaper-0-topic]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Stopped
2025-02-16 15:18:13.050 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Shutdown completed
2025-02-16 15:18:13.050 [SpringApplicationShutdownHook]  INFO kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Shutting down.
2025-02-16 15:18:13.051 [SpringApplicationShutdownHook]  INFO kafka.coordinator.transaction.TransactionStateManager - [Transaction State Manager 0]: Shutdown complete
2025-02-16 15:18:13.051 [SpringApplicationShutdownHook]  INFO kafka.coordinator.transaction.TransactionMarkerChannelManager - [TxnMarkerSenderThread-0]: Shutting down
2025-02-16 15:18:13.051 [TxnMarkerSenderThread-0]  INFO kafka.coordinator.transaction.TransactionMarkerChannelManager - [TxnMarkerSenderThread-0]: Stopped
2025-02-16 15:18:13.051 [SpringApplicationShutdownHook]  INFO kafka.coordinator.transaction.TransactionMarkerChannelManager - [TxnMarkerSenderThread-0]: Shutdown completed
2025-02-16 15:18:13.051 [SpringApplicationShutdownHook]  INFO kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Shutdown complete.
2025-02-16 15:18:13.051 [SpringApplicationShutdownHook]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Shutting down.
2025-02-16 15:18:13.052 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Shutting down
2025-02-16 15:18:13.052 [ExpirationReaper-0-Heartbeat]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Stopped
2025-02-16 15:18:13.052 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2025-02-16 15:18:13.052 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Shutting down
2025-02-16 15:18:13.052 [ExpirationReaper-0-Rebalance]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Stopped
2025-02-16 15:18:13.052 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Shutdown completed
2025-02-16 15:18:13.052 [SpringApplicationShutdownHook]  INFO kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Shutdown complete.
2025-02-16 15:18:13.053 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaManager - [ReplicaManager broker=0] Shutting down
2025-02-16 15:18:13.053 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Shutting down
2025-02-16 15:18:13.053 [LogDirFailureHandler]  INFO kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Stopped
2025-02-16 15:18:13.053 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Shutdown completed
2025-02-16 15:18:13.053 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
2025-02-16 15:18:13.053 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaAlterLogDirsManager - [ReplicaAlterLogDirsManager on broker 0] shutting down
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaAlterLogDirsManager - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Shutting down
2025-02-16 15:18:13.054 [ExpirationReaper-0-Fetch]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Stopped
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Shutdown completed
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-RemoteFetch]: Shutting down
2025-02-16 15:18:13.054 [ExpirationReaper-0-RemoteFetch]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-RemoteFetch]: Stopped
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-RemoteFetch]: Shutdown completed
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Shutting down
2025-02-16 15:18:13.054 [ExpirationReaper-0-Produce]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Stopped
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Shutdown completed
2025-02-16 15:18:13.054 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Shutting down
2025-02-16 15:18:13.055 [ExpirationReaper-0-DeleteRecords]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Stopped
2025-02-16 15:18:13.055 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2025-02-16 15:18:13.055 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Shutting down
2025-02-16 15:18:13.055 [SpringApplicationShutdownHook]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2025-02-16 15:18:13.055 [ExpirationReaper-0-ElectLeader]  INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Stopped
2025-02-16 15:18:13.060 [kafka-coordinator-heartbeat-thread | testGroup]  INFO org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Node 0 disconnected.
2025-02-16 15:18:13.061 [kafka-coordinator-heartbeat-thread | testGroup]  INFO org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Node 2147483647 disconnected.
2025-02-16 15:18:13.061 [kafka-coordinator-heartbeat-thread | testGroup]  INFO org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Node -1 disconnected.
2025-02-16 15:18:13.061 [kafka-coordinator-heartbeat-thread | testGroup]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-testGroup-2, groupId=testGroup] Group coordinator localhost:50090 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
2025-02-16 15:18:13.062 [SpringApplicationShutdownHook]  INFO kafka.server.AddPartitionsToTxnManager - [AddPartitionsToTxnSenderThread-0]: Shutting down
2025-02-16 15:18:13.062 [AddPartitionsToTxnSenderThread-0]  INFO kafka.server.AddPartitionsToTxnManager - [AddPartitionsToTxnSenderThread-0]: Stopped
2025-02-16 15:18:13.062 [SpringApplicationShutdownHook]  INFO kafka.server.AddPartitionsToTxnManager - [AddPartitionsToTxnSenderThread-0]: Shutdown completed
2025-02-16 15:18:13.062 [SpringApplicationShutdownHook]  INFO kafka.server.ReplicaManager - [ReplicaManager broker=0] Shut down completely
2025-02-16 15:18:13.062 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutting down
2025-02-16 15:18:13.062 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutdown completed
2025-02-16 15:18:13.062 [zk-broker-0-to-controller-alter-partition-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-alter-partition-channel-manager]: Stopped
2025-02-16 15:18:13.063 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerChannelManagerImpl - Node to controller channel manager for alter-partition shutdown
2025-02-16 15:18:13.063 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-forwarding-channel-manager]: Shutting down
2025-02-16 15:18:13.063 [zk-broker-0-to-controller-forwarding-channel-manager]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-forwarding-channel-manager]: Stopped
2025-02-16 15:18:13.063 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerRequestThread - [zk-broker-0-to-controller-forwarding-channel-manager]: Shutdown completed
2025-02-16 15:18:13.063 [SpringApplicationShutdownHook]  INFO kafka.server.NodeToControllerChannelManagerImpl - Node to controller channel manager for forwarding shutdown
2025-02-16 15:18:13.064 [SpringApplicationShutdownHook]  INFO kafka.log.LogManager - Shutting down.
2025-02-16 15:18:13.064 [SpringApplicationShutdownHook]  INFO kafka.log.LogCleaner - Shutting down the log cleaner.
2025-02-16 15:18:13.064 [SpringApplicationShutdownHook]  INFO kafka.log.LogCleaner$CleanerThread - [kafka-log-cleaner-thread-0]: Shutting down
2025-02-16 15:18:13.064 [kafka-log-cleaner-thread-0]  INFO kafka.log.LogCleaner$CleanerThread - [kafka-log-cleaner-thread-0]: Stopped
2025-02-16 15:18:13.064 [SpringApplicationShutdownHook]  INFO kafka.log.LogCleaner$CleanerThread - [kafka-log-cleaner-thread-0]: Shutdown completed
2025-02-16 15:18:13.084 [log-closing-/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164]  INFO org.apache.kafka.storage.internals.log.ProducerStateManager - [ProducerStateManager partition=test-topic-0] Wrote producer snapshot at offset 1 with 1 producer ids in 3 ms.
2025-02-16 15:18:13.100 [log-closing-/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164]  INFO org.apache.kafka.storage.internals.log.ProducerStateManager - [ProducerStateManager partition=__consumer_offsets-0] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms.
2025-02-16 15:18:13.110 [log-closing-/var/folders/bq/b2b2v3gn4mz4dqb7kzsq_4_m0000gn/T/spring.kafka.109c4e45-371f-4cf1-a34e-61cb9c67e53810019508897083150164]  INFO org.apache.kafka.storage.internals.log.ProducerStateManager - [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 9 with 0 producer ids in 3 ms.
2025-02-16 15:18:13.136 [SpringApplicationShutdownHook]  INFO kafka.log.LogManager - Shutdown complete.
2025-02-16 15:18:13.136 [SpringApplicationShutdownHook]  INFO kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Shutting down
2025-02-16 15:18:13.136 [SpringApplicationShutdownHook]  INFO kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Shutdown completed
2025-02-16 15:18:13.136 [controller-event-thread]  INFO kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Stopped
2025-02-16 15:18:13.137 [SpringApplicationShutdownHook]  INFO kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Stopped partition state machine
2025-02-16 15:18:13.137 [SpringApplicationShutdownHook]  INFO kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Stopped replica state machine
2025-02-16 15:18:13.137 [SpringApplicationShutdownHook]  INFO kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Shutting down
2025-02-16 15:18:13.137 [SpringApplicationShutdownHook]  INFO kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Shutdown completed
2025-02-16 15:18:13.137 [Controller-0-to-broker-0-send-thread]  INFO kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Stopped
2025-02-16 15:18:13.138 [SpringApplicationShutdownHook]  INFO kafka.controller.KafkaController - [Controller id=0] Resigned
2025-02-16 15:18:13.138 [SpringApplicationShutdownHook]  INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Shutting down
2025-02-16 15:18:13.138 [feature-zk-node-event-process-thread]  INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Stopped
2025-02-16 15:18:13.138 [SpringApplicationShutdownHook]  INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Shutdown completed
2025-02-16 15:18:13.138 [SpringApplicationShutdownHook]  INFO kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Closing.
2025-02-16 15:18:13.245 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.ZooKeeper - Session: 0x100003e249e0000 closed
2025-02-16 15:18:13.245 [Test worker-EventThread]  INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100003e249e0000
2025-02-16 15:18:13.245 [SpringApplicationShutdownHook]  INFO kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Closed.
2025-02-16 15:18:13.246 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Shutting down
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Shutdown completed
2025-02-16 15:18:13.247 [ThrottledChannelReaper-Fetch]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Stopped
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Shutting down
2025-02-16 15:18:13.247 [ThrottledChannelReaper-Produce]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Stopped
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Shutdown completed
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Shutting down
2025-02-16 15:18:13.247 [ThrottledChannelReaper-Request]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Stopped
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Shutdown completed
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2025-02-16 15:18:13.247 [ThrottledChannelReaper-ControllerMutation]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Stopped
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2025-02-16 15:18:13.247 [SpringApplicationShutdownHook]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2025-02-16 15:18:13.252 [SpringApplicationShutdownHook]  INFO kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2025-02-16 15:18:13.252 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 15:18:13.252 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 15:18:13.252 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 15:18:13.253 [SpringApplicationShutdownHook]  INFO kafka.server.BrokerTopicStats - Broker and topic stats closed
2025-02-16 15:18:13.253 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.server for 0 unregistered
2025-02-16 15:18:13.253 [SpringApplicationShutdownHook]  INFO kafka.server.KafkaServer - [KafkaServer id=0] shut down completed
2025-02-16 15:18:13.257 [ConnnectionExpirer]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - ConnnectionExpirerThread interrupted
2025-02-16 15:18:13.257 [NIOServerCxnFactory.SelectorThread-0]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - selector thread exitted run method
2025-02-16 15:18:13.257 [NIOServerCxnFactory.SelectorThread-1]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - selector thread exitted run method
2025-02-16 15:18:13.260 [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0]  INFO org.apache.zookeeper.server.NIOServerCnxnFactory - accept thread exitted run method
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.ZooKeeperServer - shutting down
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.RequestThrottler - Shutting down
2025-02-16 15:18:13.260 [RequestThrottler]  INFO org.apache.zookeeper.server.RequestThrottler - Draining request throttler queue
2025-02-16 15:18:13.260 [RequestThrottler]  INFO org.apache.zookeeper.server.RequestThrottler - RequestThrottler shutdown. Dropped 0 requests
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.SessionTrackerImpl - Shutting down
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.PrepRequestProcessor - Shutting down
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.SyncRequestProcessor - Shutting down
2025-02-16 15:18:13.260 [SyncThread:0]  INFO org.apache.zookeeper.server.SyncRequestProcessor - SyncRequestProcessor exited!
2025-02-16 15:18:13.260 [ProcessThread(sid:0 cport:50088):]  INFO org.apache.zookeeper.server.PrepRequestProcessor - PrepRequestProcessor exited loop!
2025-02-16 15:18:13.260 [SpringApplicationShutdownHook]  INFO org.apache.zookeeper.server.FinalRequestProcessor - shutdown of request processor complete
2025-02-16 15:18:13.261 [SpringApplicationShutdownHook]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Closing JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 15:18:13.263 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown initiated...
2025-02-16 15:18:13.265 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown completed.
2025-02-16 16:17:27.787 [Test worker]  INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [kr.hhplus.be.server.integration.KafkaIntegrationTest]: KafkaIntegrationTest does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-02-16 16:17:27.929 [Test worker]  INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration kr.hhplus.be.server.ServerApplication for test class kr.hhplus.be.server.integration.KafkaIntegrationTest
2025-02-16 16:17:28.268 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - Starting KafkaIntegrationTest using Java 17.0.14 with PID 3312 (started by seongdo in /Users/seongdo/Desktop/hhplus_week3/server-java)
2025-02-16 16:17:28.268 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - The following 1 profile is active: "local"
2025-02-16 16:17:28.754 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:17:28.763 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 16:17:28.775 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 4 ms. Found 0 JPA repository interfaces.
2025-02-16 16:17:28.777 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:17:28.777 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 16:17:28.975 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 196 ms. Found 9 JPA repository interfaces.
2025-02-16 16:17:29.298 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:17:29.299 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-02-16 16:17:29.316 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.CouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.316 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.UserCouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderItemJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.payment.infrastructure.PaymentJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.product.infrastructure.ProductJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.stats.infrastructure.SalesStatsJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.317 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.318 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserPointJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:17:29.318 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 10 ms. Found 0 Redis repository interfaces.
2025-02-16 16:17:29.467 [Test worker]  INFO org.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2025-02-16 16:17:29.469 [Test worker]  INFO org.testcontainers.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2025-02-16 16:17:29.474 [Test worker]  INFO org.testcontainers.DockerClientFactory - Testcontainers version: 1.20.4
2025-02-16 16:17:29.659 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2025-02-16 16:17:29.852 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2025-02-16 16:17:29.853 [Test worker]  INFO org.testcontainers.DockerClientFactory - Docker host IP address is localhost
2025-02-16 16:17:29.866 [Test worker]  INFO org.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 27.4.0
  API Version: 1.47
  Operating System: Docker Desktop
  Total Memory: 7837 MB
  Labels: 
    com.docker.desktop.address=unix:///Users/seongdo/Library/Containers/com.docker.docker/Data/docker-cli.sock
2025-02-16 16:17:29.884 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Creating container for image: testcontainers/ryuk:0.11.0
2025-02-16 16:17:29.984 [Test worker]  INFO org.testcontainers.utility.RegistryAuthLocator - Credential helper/store (docker-credential-desktop) does not have credentials for https://index.docker.io/v1/
2025-02-16 16:17:30.067 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 is starting: 06325333e3a7af7f0ccbce3ee15b637987670a4304a03f0a138f7c9d5bc7fe1d
2025-02-16 16:17:30.318 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 started in PT0.434481S
2025-02-16 16:17:30.322 [Test worker]  INFO org.testcontainers.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2025-02-16 16:17:30.322 [Test worker]  INFO org.testcontainers.DockerClientFactory - Checking the system...
2025-02-16 16:17:30.322 [Test worker]  INFO org.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2025-02-16 16:17:30.325 [Test worker]  INFO tc.mysql:8.0 - Creating container for image: mysql:8.0
2025-02-16 16:17:30.384 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 is starting: 2c154c0b4afb93607c342905b62d76cd16a98971888140b4d13a5dc9746e1da7
2025-02-16 16:17:30.502 [Test worker]  INFO tc.mysql:8.0 - Waiting for database connection to become available at jdbc:mysql://localhost:50625/hhplus using query 'SELECT 1'
2025-02-16 16:17:37.094 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 started in PT6.768947S
2025-02-16 16:17:37.094 [Test worker]  INFO tc.mysql:8.0 - Container is started (JDBC URL: jdbc:mysql://localhost:50625/hhplus)
2025-02-16 16:17:37.095 [Test worker]  INFO tc.redis:7.4.2 - Creating container for image: redis:7.4.2
2025-02-16 16:17:37.133 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 is starting: f6a1d6fcf7b0a267a6e7899b77f4482f0533346aa44c114d8536c074a38259dd
2025-02-16 16:17:37.384 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 started in PT0.288773S
2025-02-16 16:17:37.813 [Test worker]  INFO org.hibernate.jpa.internal.util.LogHelper - HHH000204: Processing PersistenceUnitInfo [name: default]
2025-02-16 16:17:37.867 [Test worker]  INFO org.hibernate.Version - HHH000412: Hibernate ORM core version 6.6.4.Final
2025-02-16 16:17:37.896 [Test worker]  INFO org.hibernate.cache.internal.RegionFactoryInitiator - HHH000026: Second-level cache disabled
2025-02-16 16:17:38.129 [Test worker]  INFO org.springframework.orm.jpa.persistenceunit.SpringPersistenceUnitInfo - No LoadTimeWeaver setup: ignoring JPA class transformer
2025-02-16 16:17:38.152 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Starting...
2025-02-16 16:17:38.245 [Test worker]  INFO com.zaxxer.hikari.pool.HikariPool - HangHaePlusDataSource - Added connection com.mysql.cj.jdbc.ConnectionImpl@470602f8
2025-02-16 16:17:38.246 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Start completed.
2025-02-16 16:17:38.315 [Test worker]  INFO org.hibernate.orm.connections.pooling - HHH10001005: Database info:
	Database JDBC URL [Connecting through datasource 'HikariDataSource (HangHaePlusDataSource)']
	Database driver: undefined/unknown
	Database version: 8.0.41
	Autocommit mode: undefined/unknown
	Isolation level: undefined/unknown
	Minimum pool size: undefined/unknown
	Maximum pool size: undefined/unknown
2025-02-16 16:17:39.274 [Test worker]  INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator - HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-02-16 16:17:39.276 [Test worker]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 16:17:39.511 [Test worker]  INFO org.springframework.data.jpa.repository.query.QueryEnhancerFactory - Hibernate is in classpath; If applicable, HQL parser will be used.
2025-02-16 16:17:39.948 [Test worker] ERROR io.netty.resolver.dns.DnsServerAddressStreamProviders - Unable to load io.netty.resolver.dns.macos.MacOSDnsServerAddressStreamProvider, fallback to system defaults. This may result in incorrect DNS resolutions on MacOS. Check whether you have a dependency on 'io.netty:netty-resolver-dns-native-macos'. Use DEBUG level to see the full stack: java.lang.UnsatisfiedLinkError: failed to load the required native library
2025-02-16 16:17:40.299 [Test worker]  INFO org.redisson.Version - Redisson 3.17.6
2025-02-16 16:17:40.385 [redisson-netty-4-6]  INFO org.redisson.connection.pool.MasterPubSubConnectionPool - 1 connections initialized for localhost/127.0.0.1:50684
2025-02-16 16:17:40.422 [redisson-netty-4-20]  INFO org.redisson.connection.pool.MasterConnectionPool - 24 connections initialized for localhost/127.0.0.1:50684
2025-02-16 16:17:41.009 [Test worker]  INFO org.springframework.validation.beanvalidation.OptionalValidatorFactoryBean - Failed to set up a Bean Validation provider: jakarta.validation.NoProviderFoundException: Unable to create a Configuration, because no Jakarta Bean Validation provider could be found. Add a provider like Hibernate Validator (RI) to your classpath.
2025-02-16 16:17:41.471 [Test worker]  INFO org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver - Exposing 1 endpoint beneath base path '/actuator'
2025-02-16 16:17:41.547 [Test worker]  INFO org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = hhplus-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-02-16 16:17:41.607 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:17:41.607 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:17:41.607 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722661606
2025-02-16 16:17:41.862 [kafka-admin-client-thread | hhplus-admin-0]  WARN org.apache.kafka.clients.admin.KafkaAdminClient - [AdminClient clientId=hhplus-admin-0] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics.
2025-02-16 16:17:41.867 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.admin.client for hhplus-admin-0 unregistered
2025-02-16 16:17:41.869 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:17:41.869 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:17:41.869 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:17:41.940 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spring-kafka-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spring-kafka-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 16:17:41.970 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 16:17:42.004 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:17:42.004 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:17:42.004 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722662003
2025-02-16 16:17:42.010 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Subscribed to topic(s): sample-topic
2025-02-16 16:17:42.017 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-order-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 16:17:42.018 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 16:17:42.021 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:17:42.021 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:17:42.021 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722662021
2025-02-16 16:17:42.025 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-order-group-2, groupId=order-group] Subscribed to topic(s): order-events
2025-02-16 16:17:42.036 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-order-group-2, groupId=order-group] Cluster ID: 76CQ6CQzT1OcZIHeGVyS1g
2025-02-16 16:17:42.041 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - Started KafkaIntegrationTest in 14.002 seconds (process running for 20.067)
2025-02-16 16:17:42.064 [Test worker]  WARN org.springframework.test.context.TestContextManager - Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener] to prepare test instance [kr.hhplus.be.server.integration.KafkaIntegrationTest@1cb4434a]
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'kr.hhplus.be.server.integration.KafkaIntegrationTest': Unsatisfied dependency expressed through field 'consumer': No qualifying bean of type 'org.apache.kafka.clients.consumer.KafkaConsumer<java.lang.String, java.lang.String>' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:788)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:768)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:146)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:399)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:143)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:98)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:260)
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:160)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$11(ClassBasedTestDescriptor.java:378)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:383)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$12(ClassBasedTestDescriptor.java:378)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:377)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$7(ClassBasedTestDescriptor.java:290)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:289)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:279)
	at java.base/java.util.Optional.orElseGet(Optional.java:364)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$6(ClassBasedTestDescriptor.java:278)
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$1(TestMethodTestDescriptor.java:105)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:104)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:128)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:128)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:160)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:160)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92)
	at jdk.proxy2/jdk.proxy2.$Proxy6.stop(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:121)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.kafka.clients.consumer.KafkaConsumer<java.lang.String, java.lang.String>' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoMatchingBeanFound(DefaultListableBeanFactory.java:2144)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1594)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1519)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:785)
	... 93 common frames omitted
2025-02-16 16:17:42.067 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  WARN org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Error while fetching metadata with correlation id 2 : {sample-topic=LEADER_NOT_AVAILABLE}
2025-02-16 16:17:42.068 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Cluster ID: 76CQ6CQzT1OcZIHeGVyS1g
2025-02-16 16:17:42.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  WARN org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Error while fetching metadata with correlation id 5 : {sample-topic=LEADER_NOT_AVAILABLE}
2025-02-16 16:17:42.403 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.403 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.403 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-order-group-2, groupId=order-group] Unsubscribed all topics or patterns and assigned partitions
2025-02-16 16:17:42.404 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.404 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.407 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:17:42.407 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:17:42.407 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 16:17:42.407 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:17:42.412 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-order-group-2 unregistered
2025-02-16 16:17:42.423 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - order-group: Consumer stopped
2025-02-16 16:17:42.443 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.444 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.444 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Unsubscribed all topics or patterns and assigned partitions
2025-02-16 16:17:42.444 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.444 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:17:42.445 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:17:42.445 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:17:42.445 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 16:17:42.445 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:17:42.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spring-kafka-group-1 unregistered
2025-02-16 16:17:42.453 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: Consumer stopped
2025-02-16 16:17:44.930 [SpringApplicationShutdownHook]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Closing JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 16:17:44.931 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown initiated...
2025-02-16 16:17:44.934 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown completed.
2025-02-16 16:21:02.071 [Test worker]  INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils - Could not detect default configuration classes for test class [kr.hhplus.be.server.integration.KafkaIntegrationTest]: KafkaIntegrationTest does not declare any static, non-private, non-final, nested classes annotated with @Configuration.
2025-02-16 16:21:02.213 [Test worker]  INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper - Found @SpringBootConfiguration kr.hhplus.be.server.ServerApplication for test class kr.hhplus.be.server.integration.KafkaIntegrationTest
2025-02-16 16:21:02.544 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - Starting KafkaIntegrationTest using Java 17.0.14 with PID 3430 (started by seongdo in /Users/seongdo/Desktop/hhplus_week3/server-java)
2025-02-16 16:21:02.544 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - The following 1 profile is active: "local"
2025-02-16 16:21:03.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:21:03.074 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 16:21:03.084 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 2 ms. Found 0 JPA repository interfaces.
2025-02-16 16:21:03.088 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:21:03.089 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-02-16 16:21:03.294 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 203 ms. Found 9 JPA repository interfaces.
2025-02-16 16:21:03.616 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Multiple Spring Data modules found, entering strict repository configuration mode
2025-02-16 16:21:03.617 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2025-02-16 16:21:03.633 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.CouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.633 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.coupon.infrastructure.UserCouponJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.633 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderItemJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.order.infrastructure.OrderJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.payment.infrastructure.PaymentJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.product.infrastructure.ProductJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.stats.infrastructure.SalesStatsJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport - Spring Data Redis - Could not safely identify store assignment for repository candidate interface kr.hhplus.be.server.apps.user.infrastructure.UserPointJpaRepository; If you want this repository to be a Redis repository, consider annotating your entities with one of these annotations: org.springframework.data.redis.core.RedisHash (preferred), or consider extending one of the following types with your repository: org.springframework.data.keyvalue.repository.KeyValueRepository
2025-02-16 16:21:03.634 [Test worker]  INFO org.springframework.data.repository.config.RepositoryConfigurationDelegate - Finished Spring Data repository scanning in 10 ms. Found 0 Redis repository interfaces.
2025-02-16 16:21:03.780 [Test worker]  INFO org.testcontainers.images.PullPolicy - Image pull policy will be performed by: DefaultPullPolicy()
2025-02-16 16:21:03.783 [Test worker]  INFO org.testcontainers.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2025-02-16 16:21:03.788 [Test worker]  INFO org.testcontainers.DockerClientFactory - Testcontainers version: 1.20.4
2025-02-16 16:21:03.964 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2025-02-16 16:21:04.166 [Test worker]  INFO org.testcontainers.dockerclient.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
2025-02-16 16:21:04.167 [Test worker]  INFO org.testcontainers.DockerClientFactory - Docker host IP address is localhost
2025-02-16 16:21:04.190 [Test worker]  INFO org.testcontainers.DockerClientFactory - Connected to docker: 
  Server Version: 27.4.0
  API Version: 1.47
  Operating System: Docker Desktop
  Total Memory: 7837 MB
  Labels: 
    com.docker.desktop.address=unix:///Users/seongdo/Library/Containers/com.docker.docker/Data/docker-cli.sock
2025-02-16 16:21:04.207 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Creating container for image: testcontainers/ryuk:0.11.0
2025-02-16 16:21:04.319 [Test worker]  INFO org.testcontainers.utility.RegistryAuthLocator - Credential helper/store (docker-credential-desktop) does not have credentials for https://index.docker.io/v1/
2025-02-16 16:21:04.395 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 is starting: 31e71aae70a969bb379f481c5c0f330bd888df051c84d3686dd39be1bba62f06
2025-02-16 16:21:04.657 [Test worker]  INFO tc.testcontainers/ryuk:0.11.0 - Container testcontainers/ryuk:0.11.0 started in PT0.449618S
2025-02-16 16:21:04.661 [Test worker]  INFO org.testcontainers.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
2025-02-16 16:21:04.661 [Test worker]  INFO org.testcontainers.DockerClientFactory - Checking the system...
2025-02-16 16:21:04.661 [Test worker]  INFO org.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
2025-02-16 16:21:04.663 [Test worker]  INFO tc.mysql:8.0 - Creating container for image: mysql:8.0
2025-02-16 16:21:04.720 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 is starting: abed375fbbe86fa7ce2fcd5d4e3d6d55bab6b21783c57a72d0fdfb40530fbf46
2025-02-16 16:21:04.901 [Test worker]  INFO tc.mysql:8.0 - Waiting for database connection to become available at jdbc:mysql://localhost:50755/hhplus using query 'SELECT 1'
2025-02-16 16:21:11.820 [Test worker]  INFO tc.mysql:8.0 - Container mysql:8.0 started in PT7.156719S
2025-02-16 16:21:11.820 [Test worker]  INFO tc.mysql:8.0 - Container is started (JDBC URL: jdbc:mysql://localhost:50755/hhplus)
2025-02-16 16:21:11.821 [Test worker]  INFO tc.redis:7.4.2 - Creating container for image: redis:7.4.2
2025-02-16 16:21:11.845 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 is starting: 2e677ba8c82ed61b5f07b5351ff75384568788d634ff1f97a6c7d1ae4456239f
2025-02-16 16:21:12.084 [Test worker]  INFO tc.redis:7.4.2 - Container redis:7.4.2 started in PT0.262657S
2025-02-16 16:21:12.488 [Test worker]  INFO org.hibernate.jpa.internal.util.LogHelper - HHH000204: Processing PersistenceUnitInfo [name: default]
2025-02-16 16:21:12.530 [Test worker]  INFO org.hibernate.Version - HHH000412: Hibernate ORM core version 6.6.4.Final
2025-02-16 16:21:12.552 [Test worker]  INFO org.hibernate.cache.internal.RegionFactoryInitiator - HHH000026: Second-level cache disabled
2025-02-16 16:21:12.750 [Test worker]  INFO org.springframework.orm.jpa.persistenceunit.SpringPersistenceUnitInfo - No LoadTimeWeaver setup: ignoring JPA class transformer
2025-02-16 16:21:12.770 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Starting...
2025-02-16 16:21:12.854 [Test worker]  INFO com.zaxxer.hikari.pool.HikariPool - HangHaePlusDataSource - Added connection com.mysql.cj.jdbc.ConnectionImpl@2215b307
2025-02-16 16:21:12.855 [Test worker]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Start completed.
2025-02-16 16:21:12.913 [Test worker]  INFO org.hibernate.orm.connections.pooling - HHH10001005: Database info:
	Database JDBC URL [Connecting through datasource 'HikariDataSource (HangHaePlusDataSource)']
	Database driver: undefined/unknown
	Database version: 8.0.41
	Autocommit mode: undefined/unknown
	Isolation level: undefined/unknown
	Minimum pool size: undefined/unknown
	Maximum pool size: undefined/unknown
2025-02-16 16:21:13.796 [Test worker]  INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator - HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-02-16 16:21:13.798 [Test worker]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 16:21:14.033 [Test worker]  INFO org.springframework.data.jpa.repository.query.QueryEnhancerFactory - Hibernate is in classpath; If applicable, HQL parser will be used.
2025-02-16 16:21:14.462 [Test worker] ERROR io.netty.resolver.dns.DnsServerAddressStreamProviders - Unable to load io.netty.resolver.dns.macos.MacOSDnsServerAddressStreamProvider, fallback to system defaults. This may result in incorrect DNS resolutions on MacOS. Check whether you have a dependency on 'io.netty:netty-resolver-dns-native-macos'. Use DEBUG level to see the full stack: java.lang.UnsatisfiedLinkError: failed to load the required native library
2025-02-16 16:21:14.779 [Test worker]  INFO org.redisson.Version - Redisson 3.17.6
2025-02-16 16:21:14.856 [redisson-netty-4-6]  INFO org.redisson.connection.pool.MasterPubSubConnectionPool - 1 connections initialized for localhost/127.0.0.1:50818
2025-02-16 16:21:14.883 [redisson-netty-4-20]  INFO org.redisson.connection.pool.MasterConnectionPool - 24 connections initialized for localhost/127.0.0.1:50818
2025-02-16 16:21:15.422 [Test worker]  INFO org.springframework.validation.beanvalidation.OptionalValidatorFactoryBean - Failed to set up a Bean Validation provider: jakarta.validation.NoProviderFoundException: Unable to create a Configuration, because no Jakarta Bean Validation provider could be found. Add a provider like Hibernate Validator (RI) to your classpath.
2025-02-16 16:21:15.869 [Test worker]  INFO org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver - Exposing 1 endpoint beneath base path '/actuator'
2025-02-16 16:21:15.933 [Test worker]  INFO org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = hhplus-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-02-16 16:21:15.995 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:21:15.995 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:21:15.995 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722875994
2025-02-16 16:21:16.234 [kafka-admin-client-thread | hhplus-admin-0]  WARN org.apache.kafka.clients.admin.KafkaAdminClient - [AdminClient clientId=hhplus-admin-0] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics.
2025-02-16 16:21:16.239 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.admin.client for hhplus-admin-0 unregistered
2025-02-16 16:21:16.240 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:21:16.241 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:21:16.241 [kafka-admin-client-thread | hhplus-admin-0]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:21:16.301 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spring-kafka-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spring-kafka-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 16:21:16.327 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 16:21:16.360 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:21:16.360 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:21:16.360 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722876360
2025-02-16 16:21:16.366 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Subscribed to topic(s): sample-topic
2025-02-16 16:21:16.373 [Test worker]  INFO org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-order-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-02-16 16:21:16.374 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 16:21:16.377 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:21:16.377 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:21:16.377 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722876377
2025-02-16 16:21:16.380 [Test worker]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-order-group-2, groupId=order-group] Subscribed to topic(s): order-events
2025-02-16 16:21:16.385 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Cluster ID: 76CQ6CQzT1OcZIHeGVyS1g
2025-02-16 16:21:16.388 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-order-group-2, groupId=order-group] Cluster ID: 76CQ6CQzT1OcZIHeGVyS1g
2025-02-16 16:21:16.391 [Test worker]  INFO kr.hhplus.be.server.integration.KafkaIntegrationTest - Started KafkaIntegrationTest in 14.069 seconds (process running for 20.13)
2025-02-16 16:21:16.394 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Discovered group coordinator localhost:9092 (id: 2147482646 rack: null)
2025-02-16 16:21:16.394 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Discovered group coordinator localhost:9092 (id: 2147482646 rack: null)
2025-02-16 16:21:16.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] (Re-)joining group
2025-02-16 16:21:16.397 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] (Re-)joining group
2025-02-16 16:21:16.429 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: need to re-join with the given member-id: consumer-spring-kafka-group-1-501d737f-1be7-4241-9e1c-ae65ca9d25d0
2025-02-16 16:21:16.429 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Request joining group due to: need to re-join with the given member-id: consumer-order-group-2-a4e0ad41-f2a0-448c-b195-478c327cef90
2025-02-16 16:21:16.429 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] (Re-)joining group
2025-02-16 16:21:16.430 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] (Re-)joining group
2025-02-16 16:21:16.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spring-kafka-group-1-501d737f-1be7-4241-9e1c-ae65ca9d25d0', protocol='range'}
2025-02-16 16:21:16.449 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-order-group-2-a4e0ad41-f2a0-448c-b195-478c327cef90', protocol='range'}
2025-02-16 16:21:16.451 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Finished assignment for group at generation 1: {consumer-spring-kafka-group-1-501d737f-1be7-4241-9e1c-ae65ca9d25d0=Assignment(partitions=[sample-topic-0])}
2025-02-16 16:21:16.451 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Finished assignment for group at generation 1: {consumer-order-group-2-a4e0ad41-f2a0-448c-b195-478c327cef90=Assignment(partitions=[order-events-0])}
2025-02-16 16:21:16.509 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-order-group-2-a4e0ad41-f2a0-448c-b195-478c327cef90', protocol='range'}
2025-02-16 16:21:16.509 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spring-kafka-group-1-501d737f-1be7-4241-9e1c-ae65ca9d25d0', protocol='range'}
2025-02-16 16:21:16.510 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Notifying assignor about the new Assignment(partitions=[sample-topic-0])
2025-02-16 16:21:16.510 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Notifying assignor about the new Assignment(partitions=[order-events-0])
2025-02-16 16:21:16.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Adding newly assigned partitions: sample-topic-0
2025-02-16 16:21:16.511 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-order-group-2, groupId=order-group] Adding newly assigned partitions: order-events-0
2025-02-16 16:21:16.523 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Found no committed offset for partition order-events-0
2025-02-16 16:21:16.523 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Found no committed offset for partition sample-topic-0
2025-02-16 16:21:16.543 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-order-group-2, groupId=order-group] Resetting offset for partition order-events-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1001 rack: null)], epoch=0}}.
2025-02-16 16:21:16.543 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting offset for partition sample-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1001 rack: null)], epoch=0}}.
2025-02-16 16:21:16.544 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - order-group: partitions assigned: [order-events-0]
2025-02-16 16:21:16.544 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: partitions assigned: [sample-topic-0]
2025-02-16 16:21:16.772 [Test worker]  INFO org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = hhplus-producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-02-16 16:21:16.773 [Test worker]  INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
2025-02-16 16:21:16.782 [Test worker]  INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=hhplus-producer-1] Instantiated an idempotent producer.
2025-02-16 16:21:16.793 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.8.1
2025-02-16 16:21:16.793 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 70d6ff42debf7e17
2025-02-16 16:21:16.793 [Test worker]  INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1739722876793
2025-02-16 16:21:16.798 [kafka-producer-network-thread | hhplus-producer-1]  INFO org.apache.kafka.clients.Metadata - [Producer clientId=hhplus-producer-1] Cluster ID: 76CQ6CQzT1OcZIHeGVyS1g
2025-02-16 16:21:16.800 [kafka-producer-network-thread | hhplus-producer-1]  INFO org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=hhplus-producer-1] ProducerId set to 0 with epoch 0
2025-02-16 16:21:16.831 [kafka-producer-network-thread | hhplus-producer-1]  INFO kr.hhplus.be.server.config.kafka.OrderEventProducer - Message sent successfully: {"orderId":1,"userId":1,"totalPaymentAmount":1000,"totalQuantity":null,"couponId":null}
2025-02-16 16:21:16.861 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO kr.hhplus.be.server.config.kafka.OrderEventConsumer - Consumed message: Order(orderId=1, userId=1, totalPaymentAmount=1000, totalQuantity=null, couponId=null)
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Revoke previously assigned partitions sample-topic-0
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-order-group-2, groupId=order-group] Revoke previously assigned partitions order-events-0
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: partitions revoked: [sample-topic-0]
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - order-group: partitions revoked: [order-events-0]
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Member consumer-spring-kafka-group-1-501d737f-1be7-4241-9e1c-ae65ca9d25d0 sending LeaveGroup request to coordinator localhost:9092 (id: 2147482646 rack: null) due to the consumer unsubscribed from all topics
2025-02-16 16:21:18.072 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Member consumer-order-group-2-a4e0ad41-f2a0-448c-b195-478c327cef90 sending LeaveGroup request to coordinator localhost:9092 (id: 2147482646 rack: null) due to the consumer unsubscribed from all topics
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Unsubscribed all topics or patterns and assigned partitions
2025-02-16 16:21:18.073 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer - [Consumer clientId=consumer-order-group-2, groupId=order-group] Unsubscribed all topics or patterns and assigned partitions
2025-02-16 16:21:18.074 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.074 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.074 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-spring-kafka-group-1, groupId=spring-kafka-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.074 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-order-group-2, groupId=order-group] Request joining group due to: consumer pro-actively leaving the group
2025-02-16 16:21:18.087 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:21:18.087 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:21:18.088 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 16:21:18.088 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:21:18.090 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-spring-kafka-group-1 unregistered
2025-02-16 16:21:18.100 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - spring-kafka-group: Consumer stopped
2025-02-16 16:21:18.389 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:21:18.389 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:21:18.389 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 16:21:18.390 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:21:18.396 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-order-group-2 unregistered
2025-02-16 16:21:18.406 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1]  INFO org.springframework.kafka.listener.KafkaMessageListenerContainer - order-group: Consumer stopped
2025-02-16 16:21:20.499 [SpringApplicationShutdownHook]  INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=hhplus-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-02-16 16:21:20.508 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-02-16 16:21:20.509 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-02-16 16:21:20.509 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-02-16 16:21:20.509 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
2025-02-16 16:21:20.510 [SpringApplicationShutdownHook]  INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for hhplus-producer-1 unregistered
2025-02-16 16:21:20.783 [SpringApplicationShutdownHook]  INFO org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean - Closing JPA EntityManagerFactory for persistence unit 'default'
2025-02-16 16:21:20.786 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown initiated...
2025-02-16 16:21:20.789 [SpringApplicationShutdownHook]  INFO com.zaxxer.hikari.HikariDataSource - HangHaePlusDataSource - Shutdown completed.
